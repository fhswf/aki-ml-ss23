{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7652a17",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2023\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ein Machine-Learning-Projekt vonAbisZ\n",
    "<h4>1. Betrachte das Gesamtbild</h4>\n",
    "<h4>2. Beschaffe die Daten</h4>\n",
    "<h4>3. Erkunde und visualisiere die Daten</h4>\n",
    "<h4>4. Bereite die Daten für Machine-Learning-Algorithmen vor</h4>\n",
    "<h4>5. Wähle ein Modell aus und trainiere es</h4>\n",
    "<h4>6. Optimiere das Modell</h4>\n",
    "<h4>7. Nimm das System in Betrieb, überwache und warte es</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PKW-Preise schätzen\n",
    "\n",
    "In diesem Arbeitsblatt wollen wir eine auf Grundlage von Daten, die auf einem Online-Marktplatz für Gebrauchtfahrzeuge [AutoScout24](https://www.autoscout24.de) gesammelt wurden, ein Modell zur Preisvorhersage für PKWs erstellen. Da unsere Zielvariable - der Preis - ein quantitatives Merkmal ist, führen wir eine Regressionsanalyse durch. Die Regression ist ein statistisches Verfahren, mit dem man eine beobachtete, abhängige Variable durch eine oder mehrere unabhängige Variablen zu erklären versucht.\n",
    "Die **Lineare Regression** ist dabei ein Sonderfall der Regressionsanalyse, bei dem zur Beschreibung der Daten ein lineares Modell herangezogen wird. Das bedeutet, die abhängige Variable wird als Linearkombination der (mit den Regressionskoeffizienten) gewichteten unabhängigen Variablen beschrieben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die verwendeten Daten wurden im Mai 2015 für ein Projekt an der ETH Zürich gesammelt.\n",
    "Der Datensatz liegt als CSV (comma-separated values) Datei vor, einem gängigen Format für tabellarische Daten.\n",
    "Wir benutzen die Python Bibliothek **Pandas**, um den Datensatz einzulesen und um ihn zu verarbeiten.\n",
    "Die Funktion `read_csv` importiert eine CSV-Datei in ein `DataFrame` Objekt.\n",
    "Da der Datensatz einige nicht-ASCII Zeichen enthält, verwenden wir das ISO-8859-1 (Latin-1) Zeichenformat beim Import.\n",
    "Mit `head(10)` geben wir die ersten 10 Zeilen des Datensatzes aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://github.com/fh-swf-hgi/ml/raw/main/u2/vw_golf_2015_complete.csv\"\n",
    "dfile = \"./vw_golf_2015_complete.csv\"\n",
    "\n",
    "if not os.path.isfile(dfile):\n",
    "    urllib.request.urlretrieve(url, dfile)\n",
    "    \n",
    "df = pd.read_csv(\"vw_golf_2015_complete.csv\", encoding = \"ISO-8859-1\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einige weitere Informationen, z.B. die Namen und Datentypen der Spalten, erhalten wir mit der Funktion `info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `describe()`erhalten wir einige Statistik-Informationen über die Spalten des Datensatzes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Datei enthält mit ca. 70.000 Datenpunkten recht viele Einträge.\n",
    "Für unser Beispiel wollen wir den Datensatz etwas reduzieren.\n",
    "Dazu wählen wir ein spezielles Automodell aus.\n",
    "In der folgenden Zelle werden die Spalten *marke* und *modell* ausgewählt und in der resultierenden Tabelle alle doppelten Einträge verworfen. Wir erhalten so einen Dataframe der alle Modelltypen einmalig enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typen = df[[\"marke\", \"modell\"]].drop_duplicates()\n",
    "df_typen.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Neue Tabelle enthält 705 unterschiedliche Typen.\n",
    "Da die Indizes aus der ursprünglichen Tabelle übernommen werden, indizieren wir den neuen Datensatz neu durch.\n",
    "Von dem resultierenden Dataframe geben wir die ersten 10 Zeilen aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typen.reset_index(drop=True, inplace=True)\n",
    "df_typen.loc[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `query(\"modell=='Golf VI'\")` wählen wir die Golf-VI Modelle aus der Liste aus und speichern die Datenpunkte in einen neuen `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_golf6 = df.query(\"modell=='Golf VI'\")\n",
    "df_golf6.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kategoriale Merkmale umwandeln\n",
    "\n",
    "Unser Datensatz enthält mehrere Spalten (Merkmale) deren Werte keine \"Zahlen\" sondern Zeichenketten sind.\n",
    "In der obigen Ausgabe des DataFrames erkennt man diese Merkmale man Datentyp (`Dtype`) `object`.\n",
    "Diese sogenannten **kategorialen Variablen** können Zeichenketten oder auch numerische Werte besitzen (beispielsweise 0 = männlich und 1 = weiblich) und kommen in Datensätzen recht häufig vor.\n",
    "Für Datenanalysen stellen kategoriale (oder *qualitative*) Variable ein gewisses Problem dar, die verwendeten statistischen Methoden erfordern es normalerweise, dass mit Merkmalen *gerechnet* wird und hierzu ist ein Zahlenwert für die einzelnen Merkmale notwendig.\n",
    "\n",
    "Man könnte auf die Idee kommen, einfach alle vorkommenden Werte einer kategorialen Variable durch zu nummerieren.\n",
    "Dies ist aber nur dann möglich, wenn die kategoriale Variable *ordinal* ist.\n",
    "Bei ordinalen Merkamlen lassen sich die Werte der Variablen in eine natürliche Reihenfolge bringen (z. B. Schulnoten von \"sehr gut\" bis \"ungenügend\" oder Dienstränge beim Militär).\n",
    "Ordnet man diesen Kategorieen entsprechende Werte zu, so bleibt die Reihenfolge in durch die Wertigkeiten erhalten.\n",
    "Häufig sind kategoriale Variable aber *nominal*, d.h. eis gibt keine natürliche Reihenfolge der Werte (z.B. Personennamen oder Steuerklassen).\n",
    "Nummeriert man die Werte eine nominalen Variablen einfach durch, führt dies in der Regel dazu, dass statistische Verfahren diese \"Ordnung\" fehlinterpretieren und damit zu falschen Schlussfolgerungen kommen.\n",
    "\n",
    "Wir müssen also in unserem Datensatz die kategorialen Variablen entweder in metrische Variablen überführen oder bei unseren Analysen ausblenden.\n",
    "Hierfür gibt es verschiedene Methoden, die wir noch näher betrachten werden.\n",
    "An dieser Stelle wollen wir eine einfache, aber in der Praxis recht häufig verwendete Technik verwenden, nämlich die sogenannte *One-Hot-Codierung*.\n",
    "Dabei werden die $N$ unterschiedlichen Werte eines Merkmals durch $N$ separate neue Merkmale $m_1, m_2, \\dots, m_N$ dargestellt.\n",
    "Besitzt ein Datenpunkt bei dem ursprünglichen Merkmal den Wert $y$, so trägt nur das neue Merkmal $m_i$ eine $1$ bei dem das $i$ für den Wert $y$ steht. Alle anderen neuen Merkmale $m_j$ des Datenpunkts tragen den Wert $0$.\n",
    "Man fasst also die Werte des ursprünglichen Merkmals als eigenständige Merkmale auf und vermerkt für jeden Datenpunkt om der Wert vorliegt oder nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unserem DataFrame überführen wir nur die kategorialen Variablen, die 10 oder wenige Werte besitzen.\n",
    "Alle anderen werden wir später verwerfen.\n",
    "Um die One-Hot-Codierung einer DataFrame Spalte zu erzeugen, verwenden wir hier die Pandas-Funktion `get_dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df_golf6\n",
    "for c in df.select_dtypes(include=['object']).columns:\n",
    "    print(c, df_num[c].nunique())\n",
    "    if df_num[c].nunique()<10:\n",
    "        df_num = pd.get_dummies(df_num, columns = [c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.drop(df_num.select_dtypes(include=['object']).columns, axis='columns', inplace=True)\n",
    "df_num.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Visualisieren und mit Statistischen Kennwerten beschreiben\n",
    "\n",
    "Als Nächstes möchten wir einen Eindruck erhalten, wie die einzelnen Merkmale miteinander zusammenhängen.\n",
    "Ist die Anzahl der Merkmale überschaubar, bietet es sich an, **Streudiagramme** (engl. *scatter plot*) für alle Paare von Merkmalen zu zeichnen.\n",
    "\n",
    "Streudiagramm werden die Elemente zweier Datenreihen paarweise (als X- und Y-Werte) in ein kartesisches Koordinatensystem eingetragen.\n",
    "Dadurch ergibt sich eine sogenannte Punktwolke, die anzeigt, in welcher Abhängigkeit die beiden Datenreihen zueinander stehen.\n",
    "\n",
    "Für Python gibt es verschiedene Bibliotheken, mit denen man Streudiagramme zeichnen kann.\n",
    "Da unsere Daten als Pandas `DataFrame` vorliegen, bietet sich in unserem Fall die Funktion `scatter_matrix` aus dem Paket `pandas.plotting` an.\n",
    "Da wir in ein kartesisches Koordinatensystem nur numerische Koordinaten eintragen können, reduzieren wir den DataFrame auf Spalten, die entweder als 64-bit `float` oder `int` Typen vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Wir selektieren einige der Spalten unseres Dataframes...\n",
    "df_scatter = df_num[['preis', 'karosserieform_Cabrio', 'erstzulassung_jahr','kilometer', 'leistung', 'tueren']]\n",
    "# ...und erzeugen die Scatter Plots\n",
    "splot = scatter_matrix(df_scatter, figsize=(10,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Abbildung zeigt nun eine Vielzahl von *Punktwolken*.\n",
    "Uns interessiert hier vor allem die erste Zeile (oder die erste Spalte, da die Graphen an der Hauptdiagonalen gespiegelt sind).\n",
    "Dort sind die Punktwolken abgebildet, bei denen die eine Datenreihe der Preis des Autos ist.\n",
    "Wir wollen später eine **Schätzfunktion** trainieren, die möglichst gut den Preis eines Gebrauchtfahrzeugs vorhersagen kann.\n",
    "Daher müssen wir herausfinden, mit welchen anderen Merkmalen der Preis möglichst eng linear *korreliert*.\n",
    "Grafisch betrachtet, suchen wir nach Punktwolken, die sich möglichst eng anhand **einer** Geraden orientieren.\n",
    "\n",
    "Erfreulicherweise sind wir nicht auf die visuelle Analyse der Daten angewiesen.\n",
    "Der **Korrelationskoeffizient** $r$ ist eine Kenngröße, die angibt, wie stark die Werte zweier Datenreihen $x$ und $y$ miteinander linear korrelieren.\n",
    "Der Wert von $r$ liegt immer in der Grenzen $[-1,1]$.\n",
    "Dabei bedeutet ein sehr niedriger (etwa $r<-0.8$) oder ein sehr hoher Wert (etwa $r>0.8$), dass die Reihen korrelieren, es also einen statistischen Zusammenhang der Datenreihen gibt.\n",
    "Ein Wert von $r$ um den Nullpunkt (etwa $-0.5<r<0.5$) bedeutet, dass es keinen signifikanten Zusammenhang gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Berechnung der **Korrelationsmatrix** mit der Pandas Funktion `corr()`, nehmen wir den Datensatz, der alle Merkmale enthält.\n",
    "Aus der berechneten Matrix selektieren wir die Spalte *Preis* und geben die Korrelationskoeffizienten in absteigend sortierter Reihenfolge aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir delektieren aus dem DataFrame df_num nur die \"numerischen\" Spalten\n",
    "df_select = df_num.select_dtypes(include=np.number)\n",
    "corr_matrix = df_select.corr()\n",
    "corr_matrix[\"preis\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da für uns die absolut größten Werte interessant sind, wenden wir die Betragsfunktion `abs()` auf die Werte an und geben die sortierte Liste erneut aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"preis\"].abs().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Berechnen Sie den Korrelationskoeffizienten $\\rho$ für die Spalten `preis` und `kilometer` \"per Hand\"**\n",
    "\n",
    "Ziehen Sie dazu zuerst aus dem `DataFrame` `temp_df` mit dem Attribut `values` die Werte der (bereinigten) Datenreihen in NumPy Arrays.\n",
    "Sie können zur Berechnung die NumPy Funktionen `mean()` (für den Mittelwert, bzw. Erwartungswert $\\mu$) und `std()` (für die Standardabweichung $\\sigma$) benutzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\rho(X,Y) = \\Large \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}$  $\\,$   mit  $\\,$   $ \\large cov(X,Y)=E\\left[ (X-\\mu_X) (Y-\\mu_Y) \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f4101e420a49f9df29c3bd9ff7cb1ca7",
     "grade": false,
     "grade_id": "cor_xy_tests",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cov_xy(x, y):\n",
    "    ''' Berechne die Kovarianz cov(x,y)\n",
    "        der beiden Merkmale x und y\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9b2c99a71d720771",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "test_x0 = np.array([  1.,  -4.,   7.,  -6., -14.,  -1.,  -3., -33.,   6.,   3.])\n",
    "test_y0 = np.array([  2.,  -7., -14.,   3.,  -9.,  19.,  14.,  -3., -11., -23.])\n",
    "\n",
    "assert cov_xy(test_x0, test_y0) == -18.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "25c2d13096074bcfc43e7b6de65bd91b",
     "grade": false,
     "grade_id": "cell-55c5c76d61197e89",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_r(x, y):\n",
    "    ''' Berechne den Korrelationskoeffizienten \n",
    "        roh(x, y) der beiden Merkmale x und y\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3419be2161b4bd21",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert np.around(calc_r(test_x0, test_y0), 4) == -0.1362"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir die Funktion `calc_r` mit unserem Datensatz ausprobieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.query(\"modell=='Golf VI'\")[[\"preis\",\"kilometer\"]].dropna()\n",
    "ty = temp_df[\"preis\"].values\n",
    "tx = temp_df[\"kilometer\"].values\n",
    "calc_r(tx,ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy stellt ebenfalls eine Funktion `corrcoef()` zur Verfügung.\n",
    "Diese stellt die Ergebnisse als Korrelationsmatrix dar, wir können sie aber ungeachtet dessen verwenden, um unser Ergebnis zu überprüfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(tx,ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exkurs: Data Science mit Orange3\n",
    "\n",
    "Wir haben nun erste Einblicke in unseren Datensatz erhalten und werden in den kommenden Schritten ein lineares Modell trainieren, mit dem wir Preisvorhersagen treffen können.\n",
    "Diese weiteren Schritte werden wir mit ebenfalls mit Python, bzw. mit den zugehörigen Bibliotheken *Pandas* und *Scikit-learn* durführen.\n",
    "\n",
    "An dieser Stelle machen wir allerdings einen kurzen Exkurs in die Datenanalyse mit grafischen Werkzeugen.\n",
    "Dazu verwenden wir das Werkzeug **Orange3**.\n",
    "\n",
    "Orange ist ein grafisches open-source Werkzeug für Data Science und Machine Learning Anwendungen.\n",
    "Es kann als eigenständige Anwendung auf Windows, Linux oder MacOS installiert werden, oder (wie in unserem Fall) als Erweiterung einer bestehenden Python Installation.\n",
    "Orange funktioniert nach dem Baukasten-Prinzip: Das Framework beinhaltet eine Reihe von Modulen für verschiedene Aufgaben, die je nach Anwendung zu einer komplexen Data Mining Pipeline zusammengeschaltet werden können.\n",
    "Gleichzeitig ist Orange erweiterbar.\n",
    "So lassen sich mit recht einfachen Mitteln eigene Module in Python entwickeln und in den Baukasten integrieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um unsere Auto-Daten in Orange3 weiter zu verarbeiten, exportieren wir den relevanten Datensatz in eine CSV-Datei.\n",
    "CSV steht für **C**omma-**S**eparated **V**alues und ist ein Standardformat um tabellarische Daten textuell zu speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select.to_csv(\"golf6.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Daten können wir nun ein einem Neuen Orange3 **Workflow** einbinden.\n",
    "\n",
    "**Aufgabe: Analysieren Sie den Datensatz in Orange3 mithilfe eines \"Linear Regression\" Modells.**\n",
    "    \n",
    "Gehen Sie in folgenden Schritten vor:\n",
    "  1. Öffnen Sie Orange3 (über Ihr Programm-Menü oder den *Anaconda Navigator*).\n",
    "  1. Wählen Sie im Welcome-Menü die Option *New Workflow*.\n",
    "  1. Wählen Sie aus dem Katalog *Data* das Modul *CSV File Import*. In den Optionen des Moduls wählen Sie die Datei \"*golf6.csv*\" aus.\n",
    "  1. Erstellen Sie eine Data Table, um den Inhalt der CSV-Datei zu betrachten.\n",
    "  1. Erstellen Sie einen *Scatter Plot* aus dem Bereich *Visualize* des Katalogs. Plotten Sie die *Kilometer* (x-Achse) gegen den *Preis* (y-Achse).\n",
    "  1. Fügen Sie ein Modul des Typs *Select Columns* ein. Wählen Sie aus den Daten die Merkmale `karosserieform_Cabrio`, `erstzulassung_jahr`, `kilometer` und `leistung` als *Features* aus, das Merkmal `preis` soll die *Target Variable* sein.\n",
    "  1. Fügen Sie ein Modul des Typs *Data Sampler* ein. 70% der Daten sollen als *Data Sample* ausgewählt werden, die restlichen 30% werden als *Remaining Data* bezeichnet.\n",
    "  1. Fügen Sie aus dem Bereich *Model* ein Modul des Typs *Linear Regression* ein und Trainieren Sie das Modell mit dem *Data Sample*.\n",
    "  1. Fügen Sie ein *Predictions* Modul (aus dem Bereich *Evaluate*) ein und verbinden SIe es mit dem *Linear Regression* Modell. Auf den Eingang *Data* legen Sie den Ausgang *Remaining Data* des *Data Sampler* Moduls.\n",
    "  1. Ihr *Workflow* sollte nun in etwa wie auf der Abbildung unten aussehen. Zu welchem Ergebnis führt das Modell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/fh-swf-hgi/ml_aki/raw/main/u2/orange3-car1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Vergleichen Sie das \"Linear Regression\" Modell mit einem \"Regression Tree\". Verwenden Sie dazu das \"Test and Score\" Modul wie auf der Abbildung unten. Welches Modell liefert die besseren Ergebnisse?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/fh-swf-hgi/ml_aki/raw/main/u2/orange3-car2a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten selektieren und aufbereiten\n",
    "Wir wollen uns für die weitere Analyse auf die Merkmale *karosserieform_Cabrio*, *erstzulassung_jahr*, *kilometer* und *leistung* konzentrieren, da sie die höchste Korrelation mit dem Merkmal *preis* aufweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golf_6_num = df_select[[\"preis\",\"karosserieform_Cabrio\",\"erstzulassung_jahr\",\"kilometer\",\"leistung\"]]\n",
    "golf_6_num.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir den Datensatz auf diese Merkmale reduzieren, fällt auf, dass nicht alle Datenreihen die gleiche Anzahl an Werten beinhalten (siehe Spalte `Non-Null`).\n",
    "Diese \"Lücken\" im Datensatz sind kritisch für die weiteren Schritte.\n",
    "Viele Algorithmen kommen mit unvollständigen Daten, bzw `NaN` Elementen nicht zurecht.\n",
    "Daher eliminieren wir die Datenpunkte, die ein `NaN` enthalten mit der Funktion `dropna()` aus dem Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golf_6_num = golf_6_num.dropna()\n",
    "golf_6_num.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir für die ausgewählten Merkmale Streudiagramme plotten, sehen wir, dass sich für den Preis Abhängigkeiten der Art **\"je mehr, desto mehr\"** (*erstzulassung_jahr*, *leistung*) sowie **\"je mehr, desto weniger\"** (*kilometer*) abzeichnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(golf_6_num, figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes wollen wir den Datensatz in 2 Teile aufteilen.\n",
    "Einen **Trainingsdatensatz** sowie einen **Testdatensatz**.\n",
    "Mit dem ersten *trainieren* wir unser Modell.\n",
    "Mit dem zweiten Datensatz testen wir die trainierte Modellfunktion.\n",
    "Mit diesem Ansatz adressieren wir das Problem des **Overfittings** (Überanpassung).\n",
    "Es kann beim Trainieren des Modells dazu kommen, dass der Lern-Algorithmus die Modell-Parameter zu sehr auf den Trainingsdatensatz anpasst.\n",
    "Durch vorhalten eines separaten Datensatzes, der nicht zum Trainieren benutzt wird, kann dieses Problem erkannt werden.\n",
    "\n",
    "Es ist üblich, etwa 1/5 bis 1/3 der Daten für den Trainingsdatensatz und die restlichen Daten für den Testdatensatz zu verwenden.\n",
    "Um die Daten aufzuteilen verwenden wir die Funktion `train_test_split` aus dem *sklearn* Modul `model_selection`.\n",
    "Mit dem Parameter `test_size` bestimmen wir den Anteil der Testdaten. Mit `random_state` wird der Zufallszahlengenerator initialisiert, über den die Datenpunkte ausgewählt werden. Legt man `random_state` fest, so wird immer die gleiche Aufteilung vorgenommen. Dies hat den Vorteil, dass die Analysen vergleichbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = golf_6_num['preis']\n",
    "# Wir wählen nur die beiden aussagekäftigsten Merkmale aus\n",
    "X = golf_6_num[[\"karosserieform_Cabrio\",\"erstzulassung_jahr\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineare Regression\n",
    "\n",
    "Bisher haben wir ausschließlich den Datensatz grafisch und statistisch analysiert, aber noch kein Machine Learning Algorithmus angewendet.\n",
    "Wir sind jetzt bereit, aus den gesammelten Daten eine *Schätzfunktion* abzuleiten.\n",
    "Diese Funktion soll auf Grundlage eines oder mehrerer Merkmale eine Schätzung für den Preis des Fahrzeugs ausgeben.\n",
    "Diese Schätzung soll so ausfallen, dass sie möglichst gut zu den bestehenden Datenpunkten passt.\n",
    "damit könnte man z.B. der *Richtpreis* für eine neue Annonce berechnen oder bestimmen, ob ein Angebot im Vergleich eher teuer oder günstig ist.\n",
    "Um diese Schätzfunktion zu entwickeln, stürzen wir uns auf die Zusammenhänge, die wir in der Analysephase herausgearbeitet haben und trainieren auf dieser Grundlage ein lineares Regressionsmodell.\n",
    "\n",
    "Es ist durchaus möglich, das Regressionsmodell mit den Mitteln, die wir bisher kennengelernt haben (NumPy, Pandas), aufzustellen und zu trainieren.\n",
    "Um die Trainingsalgorithmen kennenzulernen, werden wir das zu einem späteren Zeitpunkt auch tun.\n",
    "Allerdings ist es in der Praxis eher unüblich, Machine Learning (ML) Modelle \"per Hand\" zu entwickeln.\n",
    "Es gibt, insbesondere für Python, eine Vielzahl von Bibliotheken, mit denen ML Modelle effizient trainiert werden können.\n",
    "\n",
    "Eine weit verbreitete ML Bibliothek für Python ist **Scikit-Learn** (oder auch *sklearn*).\n",
    "Eine Besonderheit von Scikit-Learn ist, dass die Bibliothek sehr umfassend ist und ein breites Spektrum von ML-Algorithmen unterstützt.\n",
    "Darunter sind Algorithmen zur Klassifikation, Regression und für das Clustering.\n",
    "Auch unterstützende Methode wie etwa Algorithmen zur Dimensionsreduktion, zur Modellauswahl und zur Vorverarbeitung der Rohdaten sind in der Bibliothek enthalten.\n",
    "\n",
    "Die Funktionen zur linearen Regression entnehmen wir aus dem Modul `sklearn.linear_model`.\n",
    "Zur besseren Übersicht, weisen wir die abhängige Variable auf `y` (bzw. `y_test`) zu, die unabhängige Variable auf `X` (bzw. `X_test`)\n",
    "\n",
    "Wir erzeugen mit `lin_reg` ein Modell als Objekt der Klasse `LinearRegression` und trainieren das Modell mit dem Aufruf `fit(X,y)` auf unsere Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir das Modell trainiert, aber wie gut passt es zu unserem Datensatz?\n",
    "Eine Antwort aus diese Frage kann uns der durchschnittlichen Fehler für unseren Trainingsdatensatz geben.\n",
    "Die Funktion `mean_absolute_error` berechnet den Mittelwert der absoluten Fehler für die Schätzung.\n",
    "\n",
    "Damit wir mehrere Modelle schneller untersuchen können, schreiben wir eine Funktion `eval_model` die auf Grundlage der Testdaten (`X_test`) sowie der zugehörigen tatsächlichen Ergebnisse der Testdaten (`y_test`) einen *Schätzer* (`model`) untersucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "def eval_model(model, X_test, y_test):\n",
    "    avg_price = y_test.values.mean()\n",
    "    print(\"Mittlerer Preis: %d EUR\" % avg_price)\n",
    "    pred = model.predict(X_test)\n",
    "    pred_err = mean_absolute_error(y_test, pred)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    pred_err_pc = pred_err*100//avg_price\n",
    "    print(f\"Im Mittel liegt die Preisschätzung um {pred_err:.2f}€ oder {pred_err_pc:.1f}% daneben (R²={r2:.3f})\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun rufen wir `eval_model` mit unserem Modell `lin_reg` auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(lin_reg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine mehr als 10%-ige Abweichung von Preis ist nicht unbedingt gut.\n",
    "Wir können das Ergebnis verbessern, indem wir mehr Variablen in das Modell aufnehmen.\n",
    "\n",
    "**Aufgabe: Trainieren Sie ein LinearRegression-Modell mit allen Merkmalen des Datensatzes `golf_6_num`. Wie gut kann das Modell den erwarteten Preis vorhersagen?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a0a9cc44be3b882ef4a042a074aed8d8",
     "grade": true,
     "grade_id": "cell-802f4d8ae156de35",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Verwenden Sie weitere Regressionsverfahren um die Ergebnisse mit dem Linear-Regression-Modell zu vergleichen**\n",
    "\n",
    "*sklearn* bietet noch weitere Klassen für Regressionsanalysen. Darunter etwa `sklearn.linear_model.ElasticNet`, `sklearn.ensemble.GradientBoostingRegressor` und `sklearn.tree.DecisionTreeRegressor`.\n",
    "Verwenden Sie diese Modelle und vergleichen Sie die Ergebnisse mit dem Modell oben zu vergleichen. Welches Verfahren liefert die besten Ergebnisse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f9d82084ce8ce9d530ddbdae177d8517",
     "grade": true,
     "grade_id": "cell-9daa051d3f0f0d68",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
