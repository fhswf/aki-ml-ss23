{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bec6f04",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2023\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich von Klassifikationsverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden einen zufällig generierten Datensatz um die *Logistische Regression*, *Entscheidungsbäume* und *SVMs* miteinander zu vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_classes = 10 #change this\n",
    "n_data = 400 # change this\n",
    "n_dimensions = 2 # kept to two dimensions for easy visualization\n",
    "\n",
    "# generating ten-class dataset\n",
    "X, y = make_blobs(n_samples=n_data, centers=n_classes, n_features=n_dimensions, random_state=13)\n",
    "\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Trainieren Sie 3 verschiedene Modell auf den generierten Datensatz:\n",
    "- Logistische Regression (`LogisticRegression`)\n",
    "- Support Vector Classifier (`SVC`)\n",
    "- Entscheidungsbaum (`DecisionTreeClassifier`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1c82e012b55052a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-378f76ab471ccb61",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "#----------\n",
    "# Model building\n",
    "\n",
    "assert type(logreg) == LogisticRegression\n",
    "assert type(tree) == DecisionTreeClassifier\n",
    "assert type(svc) == SVC\n",
    "#----------\n",
    "\n",
    "#----------\n",
    "# Model training\n",
    "\n",
    "assert (logreg.intercept_).any(), 'Make sure to fit the data to logreg model'\n",
    "assert (tree.classes_).any() , 'Make sure to fit the data to tree model'\n",
    "assert (svc.classes_).any() , 'Make sure to fit the data to forest model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting decision regions\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "models = [logreg, svc, tree]\n",
    "names = ['Logistische Regression', 'SVC', 'Entscheidungsbaum']\n",
    "figure = plt.figure(figsize=(20, 6))\n",
    "\n",
    "for i, (name, model) in enumerate(zip(names, models)):\n",
    "    ax = plt.subplot(1, 3, i+1)\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    score = model.score(X_train, y_train)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdBu)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu)\n",
    "    plt.xlabel(\"X0\", fontsize=14)\n",
    "    plt.ylabel(\"X1\", fontsize=14)\n",
    "    plt.title(name+': %d%%' %(score*100))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Die Ergebnisse für den Entscheidungsbaum sind sehr hoch, auf den Plots sieht es aber so aus, als würden die Modelle *overfitten*. Überprüfen Sie die Classification Accuricy anhand der Testdaten.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-95d4030f59a411dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores_test = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(scores_test)==3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatsächlich liegt bei den Modellen Overfitting vor. Um diese zu umgehen, wollen wir die Tiefe des Entscheidugngsbaumes beschränken (auch *pruning* genannt). Dies wird dazu führen, dass das Modell besser verallgemeinert.\n",
    "Eine Variante, das sogenannte *Cost-Complexity Pruning*, lässt sich über den Parameter `ccp_alpha` einstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0, 0.02, 0.001) # testing alpha range\n",
    "score = []\n",
    "# iterating over different alpha values\n",
    "for alpha in alphas:\n",
    "    tree = DecisionTreeClassifier(ccp_alpha=alpha, random_state=12).fit(X_train,y_train)\n",
    "    score.append([tree.score(X_test, y_test), tree.score(X_train, y_train)])\n",
    "score = np.array(score)*100\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Entscheidungsbaum: Accuracy vs. alpha\")\n",
    "ax.plot(alphas, score[:,0],'-o', label=\"test\", drawstyle=\"steps-post\")\n",
    "ax.plot(alphas, score[:,1],'-o', label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Bestimmen Sie, an welcher Stelle (Wert für Alpha) die Accuracy am besten ist.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-77552037769c2e6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "best_score = None\n",
    "best_alpha = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('Die Vorshersagegenauigkeit ist mit %d%% am besten für alpha=%.3f' \n",
    "      % (best_score, best_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Zelle\n",
    "### BEGIN HIDDEN TEST\n",
    "assert best_alpha >= 0.006 and best_alpha < 0.0131\n",
    "### END HIDDEN TEST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
