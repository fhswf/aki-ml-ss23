{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b269c4c8",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2023\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Analysieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten sind die Grundlage von Machine Learning (ML) Algorithmen.\n",
    "Debei zählt nicht nur die Masse an Daten, sondern auch, oder vor allem deren Qualität.\n",
    "Die Ergebnisse der ML-Algorithmen können nur so gut sein, wie die Qualität der Daten es zulässt.\n",
    "\n",
    "Daher ist das Verständnis von Daten ein wesentlicher Schritt für jedes ML Projekt.\n",
    "Lassen Sie uns zunächst einige grundlegende Begriffe von Daten durchgehen:\n",
    "\n",
    "Liegen die Daten in *strukturierter* Form vor so kann man sie in der Regel als Tabelle oder Matrix beschrieben.\n",
    "Die einzelnen Zeilen dieser Tabellen nennt man **Instanzen** oder **Datenpunkte**, bei den Spalten spricht man von **Attributen**, **Merkmalen** oder **Variablen**.\n",
    "\n",
    "- Ein **Datenpunkt** ist ein Datenpaket, das ein Objekt beschreibt (einen Fall, eine Person, ein Zeitpunkt, ...).\n",
    "- Ein **Attribut** ist eine messbare Eigenschaft, anhand derer Objekte beschrieben werden (Größe, Alter, Gewicht, Augenfarbe, ...).\n",
    "\n",
    "Attribute können **kategorisch** oder **numerisch** sein:\n",
    "- **Kategorisch** sind Attribute, deren Wertebereich eine endliche Menge ist (Farbe, Typ, Wochentag, ...)\n",
    "  - **Ordinal** sind katagorische Attribute mit Ordnung (sehr schlecht, schlecht, zufriedenstellend, gut, sehr gut)\n",
    "  - **Nominal** sind katagorische Attribute ohne Reihenfolge (grün, blau, gelb)\n",
    "- **Numerisch** sind Attribute, die durch Zahlen dargestellt werden (Größe, Gewicht, Temperatur, ...) und innerhalb eines endlichen oder unendlichen Intervalls einen beliebigen Wert annehmen können.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden betrachten wir einen Datensatz für Spielerdaten aus dem FIFA 20-Videospiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"players_20.csv\", encoding = \"ISO-8859-1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`age`, `height_cm` und `weight_kg` sind Beispiele für numerische Attribute, `nationality`, `club` und `preferred_foot` sind kategorische Attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung\n",
    "Um einen besseren Eindruck über die Attribute zu erhalten, ist es ratsam, die die Daten zu visualisieren.\n",
    "Eine umfangreiche und weit verbreitete Python Bibliothek dafür ist `matplotlib`.\n",
    "\n",
    "### 1. Säulendiagramme (Bar Charts)\n",
    "Säulendiagramme sind eine einfache Möglichkeit, um die Häufigkeit bestimmter Werte in **kategorischen Merkmalen** darzustellen\n",
    "\n",
    "Im folgenden Diagramm wird gezeigt, wie häufig bestimmte Clubs im Datensatz genannt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['club'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei einer Variante der Säulendiagramme sind die \"Säulen\" horizontal aufgetragen. Man spricht in dem Fall von **Balkendiagrammen** (*column charts*). Diese haben den Vorteil, dass die zu vergleichenden Werte besser lesbar sind.\n",
    "\n",
    "**Aufgabe** Verwenden Sie die Methode `barh` der `matplotlib` Bibliothek um ein weiteres kategorisches Merkmal darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6193ce752abfa41f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Histogramm\n",
    "\n",
    "Mit einem Histogramm Häufigkeitsverteilung von werten eines **numerischen Merkmals**  grafisch dargestellt werden.\n",
    "\n",
    "Der Wertebereich des Attributs wird dazu in Intervalle (mit normalerweise gleicher Größe) eingeteilt.\n",
    "Die Anzahl der Werte, die in einen Bereich hineinfallen, bestimmt dann die Größe der Säulen in einem Säulendiagramm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['height_cm'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das obige Histogramm zeigt eine Werteverteilung, die in etw die Form einer Glockenkurve hat. Man kann also vermuten, das die Größe der Spieler eine normalverteilte Variable ist.\n",
    "\n",
    "**Aufgabe** Plotten Sie die Attribute `potential` und `overall` in ein Histogramm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b995f64b7337df78",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Streudiagramm (scatter plot)\n",
    "\n",
    "Ein Streudiagramm zeigt den Zusammenhang von Wertepaaren mehrerer Attribute.\n",
    "Der einfachste Fall ist ein zweidimensionales Diagramm in dem die Abhängigkeiten zweier Attribute dargestellt sind.\n",
    "Jeder Punkt entspricht einem Datenpunkt, wobei die *x* und *y* Koordinaten den Werten der beiden Attribute für diesen Datenpunkt entspricht.\n",
    "\n",
    "Im folgenden Diagramm vergleichen wir die Größe und das Gewicht der Spieler.\n",
    "Man sieht im Diagramm bereits eine Art *Muster*: Je größer ein Spieler ist, desto schwerer ist er **im Allgemeinen** auch.\n",
    "Wichtig ist hier: Das ist kein *Gesetz* oder eine *Regel* die immer gilt. Es gibt auch Spieler, die größer und gleichzeitig leichter als andere Spieler sind. Solche Fälle sind aber eher die Ausnahme, bzw. es ist weniger wahrscheinlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter('height_cm', 'weight_kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn mehr als 2 Attribute verglichen werden sollen, wird die Darstellung komplizierter.\n",
    "Eine Möglichkeit ist, von zwei- in den dreidimensionalen Raum zu wechseln.\n",
    "Eine andere Möglichkeit ist, das Aussehen der einzelnen Punkte (z.B. die Farbe) anhand weiterer Attribute zu variieren.\n",
    "\n",
    "\n",
    "**Aufgabe:** Verwenden Sie den Parameter `c` der `matplotlib`-Funktion `scatter`, um neben `height_cm` und `weight_kg` ein weiteres Attribut (z.B. `overall`) darzustellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ca33f9a2aa87388a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Box Plot\n",
    "\n",
    "Boxplots sind eine Methode, mit der sich die charakteristischen Eigenschaften eines numerischen Attributs auf kompakte und dennoch übersichtliche Weise darstellen lassen.\n",
    "\n",
    "Die Box entspricht dem Bereich, in dem die mittleren 50% aller Werte liegen, der Strich in der Box gibt die Position des Medians an.\n",
    "Die *Whisker* (auch *Antennen* genannt) geben den Bereich an, in dem die allermeisten Werte liegen.\n",
    "Punkte außerhalb dieses Bereichs sind als *Ausreißer* zu werten.\n",
    "\n",
    "![Box plot](https://upload.wikimedia.org/wikipedia/commons/b/b1/Elements_of_a_boxplot.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['potential','overall']].plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausreißererkennung\n",
    "\n",
    "Ein *Ausreißer* ist ein Wert, der weit entfernt von allen (oder den allermeisten) anderen Werten des gleich Datensatzes liegt.\n",
    "\n",
    "Eine Möglichkeit, solche Ausreißer zu erkennen, ist die Daten zu visualisieren, z.B. mithilfe eines Boxplots.\n",
    "Im folgenden Beispiel schauen wir uns das Merkmal `value_eur`, also den Marktwert der Spieler an.\n",
    "Wie die Angabe an der Die y-Achse zeigt, sind die Werte in $1e7$, also Millionen dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['value_eur'].plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datenpunkte, die Außreißer in mehreren Kategorien darstellen, identifiziert man auch über Streudiagramme.\n",
    "Wie wir in folgendem Beispiel sehen, besitzt ein Spieler ein sehr hohen Wert in den Kategorien *Spielerwert* (`value_eur`) und *Gehalt* (`wage_eur`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter('value_eur', 'wage_eur')\n",
    "plt.scatter(data['value_eur'][0],data['wage_eur'][0], s=150, edgecolors='k', c='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausreißer können aber auch berechnet werden.\n",
    "Im folgenden Beispiel berechnen wir für alle Datenpunkte und Merkmale, welche Werte mehr als drei Standardabweichungen größer als der Mittelwert sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 3*data.std() + data.mean()\n",
    "(data.gt(s, axis=1)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wie die Ausreißer erkannt haben, können wir sie aus dem Datensatz entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data[(data.gt(s, axis=1)).any(axis=1)==False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Der ursprüngliche Datensatz hat {data.shape[0]} Zeilen\")\n",
    "print(f\"Der bereinigte Datensatz hat noch {data_clean.shape[0]} Zeilen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenvorverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor Machine Learning Verfahren eingesetzt werden können, müssen die Datensätze in der Regel sorgfältig vorberarbeitet werden.\n",
    "Je nach verwendetem Verfahren kann es nötig sein, bestimmte Vorverarbeitungsschritte durchzuführen.\n",
    "Die im Folgenden beschrieben Schritte sind für fast alle Anwendungen notwendig:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "Datensätze sind häufig groß, in dem Sinne, dass viele Daten für verschiedene Merkmale gesammelt wurden.\n",
    "Viele Daten zu haben ist prinzipiell auch vorteilhaft, allerdings sollten für Analysen möglichst nur die *relevanten* Daten herangezogen werden.\n",
    "\n",
    "In manchen Fällen können Sie die weniger relvanten Merkale direkt identifizieren.\n",
    "In unserem Datensatz nehmen wir einmal an, dass der Spielername und die Positionen eher unwichtig sind. Daher nehmen dir die kompletten Merkmale, also die Spalten in unserem Datensatz, heraus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.drop(['short_name','player_positions'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Merkmale relevant sind und welche nicht ist allerdings nicht immer einfach zu beantworten.\n",
    "Daher ist es ratsam, mathematische Verfahren zu verwenden, mit denen weniger relevante oder sogar überflüssige Merkmale identifiziert werden können.\n",
    "Mahr dazu aber später..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisierung\n",
    "Die Wertebereiche der verschiedenen Merkmale können untereinander sehr unterschiedlich sein.\n",
    "Das Alter der Spieler wird kaum über $50$ gehen, die Gehälter hingegen fangen erst deutlich über $1000$ an.\n",
    "Wenn unser Machine Learning Modell darauf angewiesen ist, dass alle Merkmale den selben *Hebel* für das Modell haben, ist es sinnvoll, wenn die Werte aller Merkmale *normalisiert* werden.\n",
    "\n",
    "\n",
    "### 1. Standardisierung\n",
    "Ein der am häufigsten angewendeten Methoden zur Normalisieurung ist die *Standardisierung*.\n",
    "Hierbei wird von den Werten der Datenpunkte des Attributs $X$ zunächst der Mittelwert $\\bar X$ abgezogen, sodass die transformierten Daten immer den Mittelwert Null besitzen. Anschließend teilt man den Wert durch die Varianz $\\sigma^2_X$, sodass das transformierte Merkmal $\\hat X$ die Varianz 1 besitzt:\n",
    "$$\\hat x\\mapsto \\frac{x - \\bar X}{\\sigma^2_X}$$\n",
    "\n",
    "Man kann die Standardisierung nach der oben angeeben Formel in Python *per Hand* ausführen, oder bestehende Funktionen verwenden, z.B. die Funktion `scale`aus dem `preprocessing`-Modul der umfangreichen Python-Bibliothek `scikit-klearn` (kurz: `sklearn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selektiere die Spalten mit numerischen Datentypen (int64) in unserem Fall\n",
    "ncul = data_clean.select_dtypes('int64').columns\n",
    "\n",
    "# Wende die Formel zur Standardisierung an\n",
    "data_strd = (data_clean[ncul] - data_clean.mean())/data_clean.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisierung mit sklearn\n",
    "data_skstrd = scale(data_clean[ncul])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir die die Streuungswerte der neuen Datensätze vergleichen sehen wir, dass die Mittelwerte sehr nahe bei 0 und die Standardabweichung nahe bei 1 liegt.\n",
    "Außerdem sind die selbst berechneten Daten sehr nahe an den mit `sklearn` berechneten Daten.\n",
    "Unterschiede kommen aufgrund der unterschiedlichen numerischen Berechnungen zustande. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_strd.describe().loc[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_skstrd, columns=data_strd.columns).describe().loc[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Min-Max Normalisierung\n",
    "Eine, weitere Methode zur Normalisierung der Datenreihen ist die *Min-Max-Skalierung*.\n",
    "Die Idee dahinter ist überaus einfach: Zunächst wird von allen Datenpunkten $x$ des Attributs $X$ das Minimum der Attributwerte $\\min_X$ abgezogen. Nach diesem Schritt beginnen alle Wertebereiche der Attribute bei 0.\n",
    "Anschließend werden alle Werte durch die Größe des Wertebereichs $\\max_X-\\min_X$  geteilt. Danach sind alle Werte auf den Wertebereich $[0,1]$ skaliert:\n",
    "$$\\hat x\\mapsto \\frac{x-\\min_X}{\\max_X-\\min_X}$$\n",
    "\n",
    "In `sklearn` heißt die Klasse zur Min-Max Normalisierung `MinMaxScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max-Skalierung per Hand\n",
    "data_scaled = (data_clean[ncul] - data_clean[ncul].min())/(data_clean[ncul].max()-data_clean[ncul].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Min-Max-Skalierung mit sklearn\n",
    "data_skscaled = MinMaxScaler().fit_transform(data_clean[ncul])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können nun wieder beide Varianten vergleichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_skscaled.max() - data_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_scaled, columns=data_scaled.columns).max() - pd.DataFrame(data_scaled, columns=data_scaled.columns).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Normieren Sie nach den Prinzipien der Min-Max-Skalierung die Werte unseres Datensatzes auf den den Bereich $[-1,1]$ (statt wie oben auf den Wertebereich $[0,1]$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66a12eec6d027b87",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data_ex = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "data_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a9e1c1c06f3dda32",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "assert (data_ex.max() == 1).all()\n",
    "assert (data_ex.min() == -1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn` kann man dies durch Überschreiben des Attributs `feature_range` des `MinMaxScaler`-Objekts erreichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_skex = MinMaxScaler(feature_range=(-1,1)).fit_transform(data_clean[ncul])\n",
    "data_skex.min(axis=0), data_skex.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir den ursprünglichen und den normierten Datensatz zusammenfügen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating our dataframe\n",
    "data_clean[ncul] = data_scaled\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "Bisher haben wir ausschließlich die numerischen Merkmale betrachtet, aber noch nicht die kategorischen,\n",
    "Die allermeisten ML Verfahren basieren darauf, dass mit den Werten der Attribute *gerechnet* wird.\n",
    "Daher ist es in aller Regel notwendig, kategorische Merkmale in numerische zu überführen, also zu *encodieren*.\n",
    "\n",
    "### 1. Ganzzahlcodierung\n",
    "Eine Möglichkeit katagorischen Daten in numerische zu überführen besteht darin, jeder *Kategorie* einen eindeutigen (Integer-) Zahlenwert zuzuordnen.\n",
    "Diese einfache Methode ist sogar sehr sinnvoll, **aber nur, wenn die kategorialen Variablen ordinal sind**.\n",
    "Ein gutes Beispiel sind die Schulnoten *sehr gut*, *gut*, *befriedigend*, usw., denen man passenderweise die Werte $1$, $2$, $3$, usw. zuordnen, und dann mit diesen Werten auch sinnvoll rechnen kann.\n",
    "Sind **nominal**, also ohne erkennbare Ordnung kann die Ganzzahlcodierung zu schlechteren oder gänzlich **unerwarteten Ergebnissen** führen.\n",
    "Das liegt vereinfacht gesagt daran, dass die Verfahren aus der numerischen Werten ahängigkeiten herleiten, die in wirklichkeit nicht exisieren.\n",
    " \n",
    "\n",
    "In `sklearn` kann man eine Ganzzahlcodierung durch die Klasse `OrdinalEncoder` realisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spalten mit kategorischen Attributen\n",
    "ccul = ['club','nationality','preferred_foot']\n",
    "\n",
    "data_en = data_clean.copy()\n",
    "data_en[ccul] = OrdinalEncoder().fit_transform(data_clean[ccul])\n",
    "data_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist ebenfalls möglich, direkt mit `pandas` kategorische Merkmale zu enkodieren.\n",
    "Dazu setzt man den Spaltentyp auf auf `category` und wählt als *Encoding* `cat.codes`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['club'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. One-hot Codierung\n",
    "\n",
    "Wir haben ja bereits angesprochen, dass sich für die nominale Merkmale die Ganzzahlkodierung nicht eignet.\n",
    "Eine sehr verbreitete Transformation, die auch für nominale Attribute verwendet werden kann ist die *one-hot Codierung*.\n",
    "Hierbei wird ein Merkmal mit $n$ unterschiedlichen Kategorien in ein einen $n$-dimensionalen Vektor überführt.\n",
    "Jede Position dieses Vektors steht für eine bestimmte Kategorie.\n",
    "Ist für einen Datenpunkt in diesem Vektor an der Stelle $i$ eine $1$ eingetragen, so besitzt der Datenpunkt für dieses Merkmal die $i$-te Kategorie.\n",
    "\n",
    "Wie man leicht sieht, kann in diesem Vektor nur eine $1$ eingetragen sein, denn der Datenpunkt kann Maximal einer Kategorie zugeordnet sein. Alle anderen Positionen des Vektors sind $0$. Nur eine Eins, daher der Namen *one-hot*.\n",
    "\n",
    "In `sklearn` verwendet man die One-hot Codierung über die `OneHotEncoder` Klasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse=False).fit_transform(data_clean[ccul])\n",
    "onehot[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier können wir das gleiche mit `pandas` erreichen, und zwar mit der Funktion `pandas.get_dummies`.\n",
    "Jedes kategorische Merkmal wird darüber zu $n$ einzelnen Merkmalen expandiert, wobei $n$ die Anzahl der Werte ($=$ Kategorien) des Merkmals ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oh = pd.get_dummies(data_clean)\n",
    "data_oh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufteilung der Trainings- und Testdaten\n",
    "\n",
    "Eine typische Vorgehensweise, um ein Machine Learning Modell zu bewerten, ist es, das Modell mit neuen, *neuen* Daten zu testen.\n",
    "*Neu* bedeutet hier, dass die Daten für das Trainieren des Modells nicht verwendet wurden.\n",
    "Kennt man die \"Ergebnisse\" (*Label*) der Testdaten, so kann man sehr genau abschätzen, wie gut das trainierte Modell funktioniert.\n",
    "\n",
    "Aber warum benötigt man hier neue Daten? Wäre es nicht gut, wenn man diese Daten auch für das training benutzen würde, um das Modell noch besser zu entwickeln?\n",
    "Ganz im Gegenteil: Im Einsatz wird Ihr Modell immer mit unbekannten Daten arbeiten müssen. Es kommt also vor allem darauf an, wie gut das Modell *generalisiert*.\n",
    "Verwenden Sie alle Ihre Daten für das Training, so wird das Modell vielleicht sehr gute Ergebnisse liefern; aber eben nur für **diese Daten**. Es ist also *übertrainiert* (engl. *overfit*).\n",
    "\n",
    "Um *Overfitting* zu verhindern, sollten Sie also immer einen Teil des Datensatzes für das Testen reservieren.\n",
    "Dieser Teil ist üblicherweise kleiner als der Trainigsdatensatz, wie groß hängt von dem Umfang des Datensatzes ab.\n",
    "\n",
    "Für große Datensätze (z.B. mit mehr als 1 Mio. Datenpunkten) ist es angebracht, ein kleineres Testset (2%) zu verwenden.\n",
    "Bei kleineren Datensätzen ist Testdatensatz von $1/3$ bis $1/5$ der Gesamtdaten üblich.\n",
    "\n",
    "`sklearn` beinhaltet die Funktion `train_test_split` zum automatischen Aufteilen von Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data_oh, test_size=0.3)\n",
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimierungen\n",
    "\n",
    "Um die Qualität der Trainingsdaten weiter zu verbessern, können weitere Techniken zum Einsatz kommen.\n",
    "Einige von diesen Techniken sollen im FOlgenden kurz vorgestellt werden.\n",
    "\n",
    "## Deminsionalitätsreduzierung\n",
    "\n",
    "Bei Erfassen von Daten ist man mmeistens nicht \"wählerisch\". Es wird gesammelt, was erfasst werden kann und nicht so sehr darauf geachtet, welche Daten man später eventuell *benötigt*.\n",
    "Dieser Ansatz ist prinzipiell gut, lässt er doch den größten Spielraum für spätere Analysen.\n",
    "\n",
    "Wenn man nun aber ein gewisse Fragestellung mit einem Datensatz bearbeiten möchte, sind in der Regel nicht alle Attribute des Datensatzes relevant.\n",
    "Sie beim Training beizubehalten bedeutet aber meistens, einen höheren Zeit- und Ressourcenaufwand beim Training der sich nicht selten in einem schlechter trainierten Modell wiederspiegelt.\n",
    "Es ist also empfehlenswert, die Anzahl der Attribute bzw. die *Dimensionalität* des Datensatzes zu reduzieren.\n",
    "\n",
    "Der bekannteste Algorithmen zur Reduzierung der Dimensionalität, ist die sogenannte Hauptkomponentenanalyse, oder auf Englisch **Principle Component Analysis (PCA)**. PCA ist eine Methode aus der Statistik, mit der man aus Daten mit vielen Merkmalen einige Faktoren extrahiert, die für diese Merkmale bestimmend, bzw. am meisten aussagekräftig sind. PCA kann nicht nur zur Reduzierung von Eingaben verwendet, sondern auch zur Visualisierung von Daten mit hoher Dimension in einer 2D-Grafik.\n",
    "\n",
    " Wie PCA genau funktioniert, werden wir an dieser Stelle nicht näher beleuchten, aber wir wollen die PCR-Methode auf unser Daten anwenden und die Ergebnisse darstellen.\n",
    " `sklearn` stellt die Klasse` PCA` bereit, die Attribute auf eine vorgegebene Zahl `n_components` reduzieren kann. Wir werden unseren Datensatz damit in nur 2 Dimensionen darstellen:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(data_clean[ncul])\n",
    "data_pca = pca.transform(data_clean[ncul])\n",
    "data_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben mit dem obigen Code die Dimensionen unseres Datensatzes von 7 auf 2 Reduziert, ohne dabei ein größeres Maß and *Informationen* (oder mathematisch gesehen an *Varianz*) zu verlieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = pca.explained_variance_ratio_*100\n",
    "print('Die erste Dimension repräsentiert %.2f%% der uerspruenglichen Varianz' %var[0])\n",
    "print('Die erste Dimension repräsentiert %.2f%% der uerspruenglichen Varianz' %var[1])\n",
    "print('Also sind %.2f%% der urspruenglichen Varianz (=Information) erhalten geblieben.' %var.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Men den nunmehr noch 2 Merkmalen lassen sich die Daten im 2D-Raum plotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_pca[:,0], data_pca[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening\n",
    "\n",
    "Das sogenannte *Whitening* ist eine lineare Transformation die mithilfe einer Hauptkomponentenanalyse durchgeführt werden kann.\n",
    "Dabei wird die Varianz der Hauptkomponenten auf $1$ normiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachten wir die Merkmale `height_cm` und `weight_kg` aus unserem Datensatz. Wir sehen, dass wir die Werte unser Datenpunkte bereits *normiert* haben.\n",
    "In beiden Kategorien ist der Wertebereich zwischen $0.0$ und $1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_clean['height_cm'], data_clean['weight_kg'])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir nun die PCA anwenden, beobachten wir eine sehr unterschiedliche Varianz der Hauptkomponenten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_wh = PCA(whiten=False).fit_transform(data_clean[['height_cm', 'weight_kg']])\n",
    "\n",
    "print(\"Varianz der Hauptkomponenten:\", pca_wh.std(axis=0)**2)\n",
    "plt.scatter(pca_wh[:,0], pca_wh[:,1])\n",
    "plt.xlabel(\"HK1\")\n",
    "plt.ylabel(\"HK2\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führt man die PCA mit  Pre-Whitening aus, wird die Varianz der Hauptkomponenten auf 1 normiert.\n",
    "Dies kann hilfreich für weitere Verarbeitungschritte der Daten sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_wh = PCA(whiten=True).fit_transform(data_clean[['height_cm', 'weight_kg']])\n",
    "\n",
    "print(\"Varianz der Hauptkomponenten:\", pca_wh.std(axis=0)**2)\n",
    "plt.scatter(pca_wh[:,0], pca_wh[:,1])\n",
    "plt.xlabel(\"HK1\")\n",
    "plt.ylabel(\"HK2\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Übung\n",
    "Erproben Sie die vorgestellten Schritte zur Datenvorverarbeitung anhand eines einfachen Datensatzes.\n",
    "Wir wollen als Beispiel den bekannten `iris` Datensatz verwenden.\n",
    "Dabei handelt es sich um einen Datensatz mit 150 Beobachtungen von 4 Attributen von insgesammt drei Schwertlilienarten (*Iris setosa*, *Iris virginica* und *Iris versicolor*).\n",
    "\n",
    "Die  Attribute sind jeweils die Breite und die Länge des Kelchblatts (*Sepalum*) und des Kronblatts (*Petalum*).\n",
    "Aufgrund dieser 4 Werte lassen sich die Schwertlilienarten recht gut *klassifizieren*.\n",
    "Der Datensatz wird in vielen Beispielen zum Thema *Data Science* verwendet und ist eine Art *Hello World* des Machine Learnings.\n",
    "\n",
    "Wir laden nun den Datensatz über die *Scikit-Learn* Funktion `sklearn.datasets.load_iris` herunter.\n",
    "Danach führen Sie bitte folgende Schritte eigenständig aus:\n",
    "1. Normalisieren des Datensatzes `X` mit der Min-Max-Normalisierung\n",
    "2. Encodieren der Zielvariablen `y` mit der One-Hot Codierung\n",
    "3. Aufteilen des Datensatzes in Trainings- und Testdaten\n",
    "4. Reduzieren des Datensatzes `X` auf 2 Attribute mit PCA und Whitening\n",
    "5. Ploten des Vorverarbeiteten Datensatzes `X`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target.reshape(-1,1) # rehape, um aus dem 1D Zeilenvektor einen Spaltenvektor zu machen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1: Normalisieren\n",
    "Normalisieren Sie `X` mit der **Min-Max-Normalisierung**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1fe73c54f740280a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_norm = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-61592fa23ac54864",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "assert type(X_norm) == np.ndarray, 'X_norm should be a numpy array containing transformed output'\n",
    "assert X_norm.ptp() == 1.\n",
    "assert X_norm.shape == X.shape\n",
    "assert (X_norm>=0).all(), 'All values must be positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2: Encoding\n",
    "Encodieren Sie `y` mit der **One-Hot Codierung**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-89e062c358c19c5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_en = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-895432b5eb2d1c4e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "assert type(y_en) == np.ndarray, 'y_en should be a numpy array containing transformed output'\n",
    "assert y_en.shape == (150, 3), 'There should be 3 columns for the 3 classes'\n",
    "assert y_en.sum() == 150.\n",
    "assert y_en.ptp() == 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 3: Splitting\n",
    "Teilen Sie `X_norm` und `y_en` in einen Test- (`X_test`, `y_test`) und einen Trainingsdatensatz (`X_train`, `y_train`) auf. Der Trainingsdatensatzsoll 70% der Datenpunkte enthalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6715bd44ba5cb27",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = [None]*4\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-93f0f1c936fd2a96",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "assert X_train.all() in X_norm, 'X_train data is not in X_norm'\n",
    "assert X_train.shape[0] == X_norm.shape[0]*0.7, 'The size of training set is not matching 70%'\n",
    "assert X_train.shape[0]+X_test.shape[0] == X_norm.shape[0]\n",
    "assert y_train.all() in y_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 4: Dimensionsreduktion\n",
    "Reduzieren Sie mittels der Hauptkomponentenanalyse die Datensätze  `X_train` und `X_test` jeweils auf 2 Attribute. Verwenden Sie dabei Whitening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-119aa25e921278f2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train2d = None\n",
    "X_test2d = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-94f1aaad1d1f4ff4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "assert type(X_train2d) == np.ndarray, 'X_train2d should be a numpy array containing transformed output, not the model'\n",
    "assert X_train2d.shape == (105, 2), 'The number of attributes is not 2'\n",
    "assert X_test2d.shape == (45, 2), 'The number of attributes is not 2'\n",
    "assert np.allclose(X_train2d.std(axis=0).ptp(), 0), 'Attributes have different variances'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 5: Visualization\n",
    "Plotten Sie den vorverarbeiteten Trainingsdatensatz `X_train2d` mit der Funktion `plt.scatter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d666ac76f8b1b04d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 6: Klassifikation\n",
    "\n",
    "Wir können nun ein Klassifikationsmodell trainieren, mit dem wir vorhersagen können, zu welcher Lilienart neue Datenpunkte zählen \n",
    "Wir verwenden hier einen Entscheidungsbaum (`DecisionTreeClassifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train2d, y_train)\n",
    "predictions = classifier.predict(X_test2d)\n",
    "print(f\"Classification Accuracy: {accuracy_score(predictions, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Reduktion der Merkmale von 4 auf 2 war in diesem Fall nicht unbedingt notwendig.\n",
    "Wie hoch ist die *Classification Accuracy*, wenn wir alle Merkmale des Datensatzes verwenden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3e99b98e0acffe8f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen:\n",
    "[1] M. Berthold, C. Borgelt, F. Höppner and F. Klawonn, Guide to Intelligent Data Analysis, London: Springer-Verlag, 2010.  \n",
    "[2] J. VanderPlas, Python Data Science Handbook, O'Reilly Media, Inc., 2016.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
