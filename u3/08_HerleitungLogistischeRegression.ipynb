{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff391a3",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2023\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Klassifikation mit Logistischer Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bisher haben wir gesehen, wie man mit der linearen Regression Schätzfunktionen für lineare Modelle aufstellen und mithilfe des Gradientenverfahrens trainieren kann.\n",
    "In diesem Arbeitsblatt wollen wir nun das Problem der Klassifikation betrachten.\n",
    "Beispiele für Klassifikationsaufgaben sind:\n",
    "- Werbung/Marketing: Wird ein Kunde ein bestimmtes Produkt kaufen?\n",
    "- Qualitätssicherung: Ist ein bestimmtes Produkt ok oder defekt?\n",
    "- Objekterkennung: Ist ein bestimmtes Objekt auf einem Bild zu sehen?\n",
    "- Betrugserkennung: Liegt bei einer bestimmten Transaktion ein Betrugsfall vor?\n",
    "- Finanzanalysen: Zahlt ein Kreditnehmer einen Kredit vollständig zurück?\n",
    "- Medizin: Hat ein Patient eine bestimmte Krankheit?\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Anders als bei der Regression, wo wir für einen neuen Datenpunkt einen möglichst genauen Schätzwert für eine abhängige Variable berechnet haben, geht es bei der Klassifikation darum, vorherzusagen, zu welcher Klasse der Datenpunkt gehört.\n",
    "Statt einer kontinuierlichen Zielgröße wird also bei der Klassifikation eine diskrete abhängige Variable vorhergesagt.\n",
    "Im einfachsten Fall ist die zu erklärende Variable binär, die Schätzungen haben also nur die Werte *ja* oder *nein*, bzw. `1` oder `0`.\n",
    "Bei der *Multiklassen-Klassifikation* kann die Zielvariable mehr als 2 diskrete Werte annehmen, die jeweils eine bestimmte Klasse kodieren.\n",
    "Beispielsweise kann ein Bilderkennungssystem vorhersagen, welches von 100 bekannten Objekten sich am wahrscheinlichsten auf einem Bild befindet.\n",
    "Werden für die Schätzung der Zielgröße, wie es üblicherweise der Fall ist, mehrere unabhängige Variable herangezogen, spricht auch von einer multivariaten Klassifikation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Grundsätzlich kann man ein Klassifikationsproblem auch mithilfe der *linearen Regression* angehen.\n",
    "Z.B. indem man eine Schätzfunktion $f$ für eine binäre Zielvariable erstellt wobei man für die Vorhersage den Wert $f(x)$ als Wahrscheinlichkeit für die Zugehörigkeit  des Datenpunkt $x$ zur Klasse `1` interpretiert. D.h., die Werte $f(x)<0.5$ werden als `0`, die Werte $f(x)\\ge0.5$ als `1` interpretiert. \n",
    "Allerdings ergeben sich dadurch einige Probleme, die die lineare Regression für Klassifikationsaufgaben nicht sehr praktikabel machen.\n",
    "U.a. liefert die Schätzfunktion $f$ auch Werte kleiner `0` und größer `1`, was bei der Interpretation als Wahrscheinlichkeit widersinnig ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Die logistische Regression löst das Problem der Schätzfunktion indem Sie das Ergebnis der linearen Funktion durch eine geeignete Transformation auf den Wertebereich `0` bis `1` abgebildet.\n",
    "Diese Transformation wird bei logistische Regression der logistischen Funktion (auch *Sigmoidfunktion* oder *S-Funktion*) durchgeführt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistische Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Um die Methode der logistischen Regression genauer zu erklären, generieren wir uns einenen synthetischen Datensatz mit nur zwei Merkmalen.\n",
    "Diese vereinfachte Problemstellung erlaubt es uns, Daten und Funktionen im zweidimensionalen Koordinatensystem zu plotten und so besser zu visualisieren.\n",
    "\n",
    "Unser frei erstellter Datensatz soll einen Zusammenhang zwischen der Zeit, die ein Student für eine Prüfung lernt und dem Prüfungsresultat beschreiben.\n",
    "Wir verwenden zur Erzeugung der Datenpunkte die Sigmoidfunktion `sigma(x)`, die auch später die Grundlage des logistischen Regressionsmodells ist.\n",
    "\n",
    "Die Sigmoidfunktion kann in Python wiefolgt implementiert werden.\n",
    "(Der Zusatz @np.vectorize bewirkt, dass die Funktion bei Eingabe eines NumPy Arrays für alle Elemente des Arrays in vektorisierter Form ausgeführt wird.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "@np.vectorize\n",
    "def sigma(x):\n",
    "    return 1.0 / (1.0+np.exp(-x))\n",
    "\n",
    "xx = np.arange(-10, 10, .1)\n",
    "\n",
    "fig, ax =plt.subplots(1, 1, figsize=(9,5))\n",
    "\n",
    "plt.plot(xx, sigma(xx), linewidth=4)\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['left'].set_position(('data',0))\n",
    "ax.spines['bottom'].set_position(('data',0))\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "    label.set_fontsize(12)\n",
    "    label.set_bbox(dict(facecolor='white', edgecolor='None', alpha=0.65 ))\n",
    "\n",
    "\n",
    "plt.savefig(\"LogistischeRegression01.png\",transparent=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Um die Verteilung der Datenpunkte etwas zu variieren, addieren wir zum Ergebnis der Sigmoidfunktion noch eine normalverteilte Störgröße `sigma_z_noise`.\n",
    "\n",
    "Der zugehörige Graph zeigt die Resultate (`0=`*nicht bestanden* und `1=`*bestanden*) auf der y-Achse und die Anzahl der Lernstunden auf der y-Achse.\n",
    "Wie man sieht, ist es nicht möglich, die eine Trennlinie, nur in Anhängigkeit von dem Merkmal *Lernstunden* zu ziehen, die die Datenpunkte in die Klassen *nicht bestanden* und *bestanden* aufteilt. Ein Wert von ca. 30 scheint eine gute Wahl zu sein, aber auch bei dieser Aufteilung gibt es einige Datenpunkte, die in die \"falsche\" Kategorie fallen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(12)\n",
    "#np.random.seed(12)\n",
    "\n",
    "@np.vectorize\n",
    "def bestehen(x):\n",
    "    z = x/10-3\n",
    "    sigma_z = 1.0 / (1.0+np.exp(-z))\n",
    "    sigma_z_noise = sigma_z + np.random.normal(0,.4)\n",
    "    #sigma_z_noise = sigma_z + np.random.normal(.1,.2)\n",
    "    if sigma_z_noise < 0.3: return 0\n",
    "    else: return 1\n",
    "\n",
    "X = np.random.uniform(0,80,40)\n",
    "y = bestehen(X)\n",
    "plt.scatter(X[y==0], y[y==0], marker='o')\n",
    "plt.scatter(X[y==1], y[y==1], marker='d')\n",
    "plt.xlabel(\"Lernstunden\", fontsize=14)\n",
    "plt.text(3, .9, \"bestanden\")\n",
    "plt.text(50, .05, \"nicht bestanden\")\n",
    "plt.axvline(x=30, c='grey', ls=':', label=\"Entscheidungsgrenze\")\n",
    "plt.savefig(\"LogistischeRegression03.png\",transparent=True, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Da wir noch nicht wissen, wie wir eine Funktion herleiten können, die auf Grundlage der Lernstunden die Wahrscheinlichkeitsverteilung für das Bestehen der Klausur voraussagt, überlegen wir, wie so eine Funktion aussehen könnte.\n",
    "Im Diagramm unten abgebildet, ist der Graph der Funktion `prob`, die eine Schätzung der Wahrscheinlichkeitsverteilung für das Bestehen der Klausur unter Angabe der Lernstunden darstellt.\n",
    "\n",
    "Diese partiell lineare Funktion ist ein mögliches Modell für die Wahrscheinlichkeiten eines Erfolgs.\n",
    "Wenn man nun die Entscheidungsgrenze bei `prob(x)=0.5` anlegt sieht man, dass die Funktion suboptimal ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3\n",
    "\n",
    "miny = X[y==1].min()\n",
    "maxy = X[y==0].max()\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def prob(x,a,b):\n",
    "    a = a-1\n",
    "    b = b+1\n",
    "    if x<a: return 0.0+epsilon\n",
    "    elif x>b: return 1.0-epsilon\n",
    "    else:\n",
    "        res = 1/(b-a) * x - (a/(b-a))\n",
    "        assert res<1, \"Für x=%f ergibt die W'keit 1\" % x\n",
    "        return res\n",
    "    \n",
    "xx = np.linspace(-10,100,100)\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"Lernstunden\", fontsize=14)\n",
    "plt.plot(xx, prob(xx,miny,maxy), c='r')\n",
    "plt.axvline(x=miny+(maxy-miny)*.5, c='grey', ls=':')\n",
    "#plt.axhline(y=.5, xmin=.0, xmax=.5, c='black', ls='--')\n",
    "\n",
    "plt.savefig(\"LogistischeRegression04.png\",transparent=True, dpi=300)\n",
    "plt.show()\n",
    "miny, maxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden nun unsere geschätzte Wahrscheinlichkeitsverteilung, um die *Chancen* (auf einen Erfolg) zu berechnen.\n",
    "In der Statistik beschreibt die Chance (engl *odds*) den Quotienten aus der Wahrscheinlichkeit $p$ eines Ereignisses und seiner Gegenwahrscheinlichkeit: $\\frac{p}{1-p}$.\n",
    "\n",
    "Beim Münzwurf beträgt die Chance z.B.  1:1 (Wahrscheilichkeit $\\frac{0,5}{0,5}$ oder \"ein guter Fall, ein schlechter Fall\"), beim Würfeln einer sechs 1:5.\n",
    "\n",
    "Bei einer Wahrscheinlichkeit von 0 ist die Chance ebenfalls 0 ($\\frac{0}{1}$). Je weiter sich die Wahrscheinlichkeit für einen Erfolg der 1 nähert, desto größer wird die Chance: $\\lim\\limits_{p \\rightarrow 1}{\\frac{p}{1-p}}=\\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def chance(x):\n",
    "    assert x<1, \"Für x=%f sind die Chancen nicht definiert\" % x\n",
    "    return x/(1-x)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax2 = ax1.twiny()\n",
    "\n",
    "xx = np.linspace(miny+epsilon,maxy-epsilon,100)\n",
    "ax1.scatter(X, y)\n",
    "\n",
    "xxx = prob(xx,miny,maxy)\n",
    "\n",
    "ax1.set_xlabel(\"Lernstunden\", fontsize=14)\n",
    "ax2.set_xlabel(r\"$p$\", fontsize=14)\n",
    "ax2.plot(xxx, chance(xxx), c='r', label=\"Chance\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig(\"LogistischeRegression_Chance.png\",transparent=True, dpi=300)\n",
    "ax1.axis([0,80,-0.1,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Im obigen Graph sind auf der x-Achse die Lernstunden für die Datenpunkte sowie die Bestehens-Wahrscheinlichkeit $p$ für die Funktion *Chance* aufgetragen.\n",
    "Unabhängig von dem Anwendungsfall besitzt *Chance* für jede lineare Wahrscheinlichkeitsfunktion $P(X=x_i)$ die gleiche Form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun transformieren wir die Chance-Funktion mit dem (natürlichen) Logarithmus aus dem Wertebereich $[0,\\infty[$ in den Bereich $]-\\infty,\\infty[$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax2 = ax1.twiny()\n",
    "\n",
    "ax1.scatter(X, y)\n",
    "ax1.set_xlabel(\"Lernstunden\", fontsize=14)\n",
    "ax2.set_xlabel(r\"$p$\", fontsize=14)\n",
    "xxx = prob(xx,miny,maxy)\n",
    "ax2.plot(xxx, chance(xxx), c='r', label=\"Chance\")\n",
    "ax2.plot(xxx, np.log(chance(xxx)), c='g', label=\"logit\")\n",
    "plt.legend(loc='upper left')\n",
    "ax1.axis([0,80,-5,5])\n",
    "plt.savefig(\"LogistischeRegression06.png\",transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die resultierende **logit** Funktion läuft \"in der Mitte\", also ca. dem Bereich $[0.2,0.8]$ annähernd linear.\n",
    "Daher können wir die Kurve durch ein lineares Modell approximieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = prob(X,miny,maxy)\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "\n",
    "xxx = prob(X,miny,maxy)\n",
    "XX = X.reshape(-1, 1)\n",
    "yy = np.log(chance(xxx)).reshape(-1, 1)\n",
    "    \n",
    "linreg.fit(XX,yy)\n",
    "t0 = linreg.intercept_\n",
    "t1 = linreg.coef_\n",
    "t0, t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im unten angegebenen Graph sind *Chance* und *logit* Funktionen der Wahrscheinlichkeit.\n",
    "Die Funktion *model* hingegen, hängt von den Lernstunden ab.\n",
    "Man sieht, dass sich die Modellfunktion und die Logit-Funktion im Mittelteil recht gut überdecken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax2 = ax1.twiny()\n",
    "ax1.set_xlabel(\"Lernstunden\", fontsize=14)\n",
    "ax2.set_xlabel(r\"$p$\", fontsize=14)\n",
    "\n",
    "ax1.scatter(X, y)\n",
    "xxx = prob(xx,miny,maxy)\n",
    "ax2.plot(xxx, chance(xxx), c='r', label=\"Chance\")\n",
    "ax2.plot(xxx, np.log(chance(xxx)), c='g', label=\"logit\")\n",
    "modely = list(map(lambda x: (t0+t1*x).item(0), X))\n",
    "modelx, modely = zip(*sorted(zip(X, modely)))\n",
    "ax1.plot(modelx, modely, c='orange', label=\"model\", linewidth=3)\n",
    "ax2.legend(loc='upper left')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.axis([0,80,-5,5])\n",
    "plt.savefig(\"LogistischeRegression07.png\",transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben bisher mit der \"selbst ausgedachten\" Verteilungsfunktion für die Wahrscheinlichkeiten *prob* gearbeitet.\n",
    "Durch Anwenden der Logit-Funktion auf *prob* haben wir eine Funktion erzeugt, die wir mit mit einem linearen Model $\\Theta^Tx$ approximieren können.\n",
    "\n",
    "Wir wollen nun betrachten, wie man die Wahrscheinlichkeitsfunktion allgemein bestimmen kann.\n",
    "Wir wissen, dass sich die Logit-Funktion $logit(x)=ln(chance(x))$ äquivalent zu der lineatren Funktion $h_{\\Theta}(x)=\\Theta^Tx$ verhält. Wir können nun durch Einsetzen und Umformen die Verteilungsfunktion $p(x)$ herleiten:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ln(chance(x)) \\thicksim \\Theta^Tx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow chance(x) \\thicksim e^{\\Theta^Tx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow \\frac{p(x)}{1-p(x)} \\thicksim e^{\\Theta^Tx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow \\frac{p(x)}{1-p(x)} \\cdot \\frac{1/p(x)}{1/p(x)} \\thicksim e^{\\Theta^Tx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow \\frac{1}{p(x)^{-1}-1} \\thicksim e^{\\Theta^Tx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow p(x)^{-1}-1 \\thicksim e^{-\\Theta^Tx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow p(x)^{-1} \\thicksim 1+e^{-\\Theta^Tx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\Leftrightarrow p(x) \\thicksim \\frac{1}{1+e^{-\\Theta^Tx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Damit haben wir gezeigt, dass sich die Wahrscheinlichkeitsverteilung als Sigmoidfunktion in Abhängigkeit der Modellparameter $\\Theta$ sowie den Werten der unabhängigen Variablen darstellen lässt.\n",
    "Damit haben wir die **Modellfunktion** für unser Klassifikationsproblem erhalten:\n",
    "\n",
    "$$\n",
    "h_{\\Theta}(x) = \\frac{1}{1+e^{-\\Theta^Tx}}\n",
    "$$\n",
    "\n",
    "Als nächsten Schtritt wollen wir nun betrachten, welche **Kostenfunktion** wir zur Bestimmung der Parameter $\\Theta$ anwenden können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bei der linearen Regression haben wir als Kostenfunktion die Summe der Fehlerquadrate verwendet.\n",
    "Schauen wir uns zunächst an, welche Kostenfunktion dieser Ansatz für die Sigmoidfunktion liefert.\n",
    "\n",
    "Wir erweitern zuerst unsere Matrix `X` um eine Spalten mit Einsen für die Bestimmung des Bias-Parameters $\\Theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "vone = (np.ones(len(X))).reshape(-1,1)\n",
    "XX = X.reshape(-1,1)\n",
    "XX = np.concatenate((vone, XX), 1)\n",
    "yy = y.reshape(-1,1)\n",
    "# Und wieder zurück:\n",
    "#X = X[:,1:]\n",
    "#X = X.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Im folgenden Code-Abschnitt wird die Kostenfunktion $J_{\\Theta}(x)$ geplottet.\n",
    "Um die Funktion 2-dimensional darstellen zu können, setzen wir einen Parameter $\\Theta_0$ fest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def h(X,theta):\n",
    "    #print(\"X:\", np.shape(X), \"Theta:\", np.shape(theta))\n",
    "    r = 1.0 / (1.0+np.exp(-(X@theta)))\n",
    "    #print(\"r:\", np.shape(r))\n",
    "    return r\n",
    "\n",
    "def J(X,theta,y):\n",
    "    #print(\"X:\", np.shape(X), \"Theta:\", np.shape(theta), \"y:\", np.shape(y))\n",
    "    j = (h(X,theta)-y).T@(h(X,theta)-y)\n",
    "    #print(\"j:\", np.shape(j))\n",
    "    return j\n",
    "\n",
    "yt1 = []\n",
    "xt1 = []\n",
    "\n",
    "for t in np.linspace(-2,2,1000):\n",
    "    xt1.append(t)\n",
    "    theata0 = np.array([33, t]).reshape((2,1))\n",
    "    yt1.append(J(XX,theata0,yy).item(0))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(xt1, yt1, label=r'$J_{\\Theta}(x)$')\n",
    "plt.legend(loc='upper right', prop={'size': 16})\n",
    "plt.xlabel(r'$\\Theta_1$', fontsize=16)\n",
    "plt.text(0, 22, r'$J_{\\Theta}(x)$ ist nicht konvex', fontsize=14)\n",
    "plt.savefig(\"LogistischeRegression08.png\",transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Man erkennt direkt, dass lokale Minimima existieren und dass diese Funktion $J$ damit nicht konvex sein kann.\n",
    "Aus diesem Grund lässt sich das Gradientenverfahren nicht auf die Kostenfunktion anwenden.\n",
    "Je nachdem, wo man mit der Parameteroptimierung startet, könnte die Suche in ein lokales Minimum laufen und damit die optimalen Modellparameter nicht finden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Da die Methode der Fehlerquadrate nicht zielführend ist, verwendet man der logistischen Regression eine andere Kostenfunktion.\n",
    "\n",
    "Die Funktion $\\hat{J}_{\\Theta}(x)$ verwendet die Logarithmusfunktion angewendet auf $h_{\\Theta}(x)$ und in Abhängigkeit von $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{J}_{\\Theta}(x)=\\begin{cases}\n",
    "-\\log (h_{\\Theta}(x)) & \\text{falls } y=1\\\\\n",
    "-\\log (1-h_{\\Theta}(x)) & \\text{falls } y=0\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xx = np.arange(0.01, 1, .01)\n",
    "\n",
    "fig, axs =plt.subplots(1, 2, figsize=(9,5))                    \n",
    "\n",
    "\n",
    "axs[0].plot(xx, -np.log(xx), linewidth=4, label=r'$J_{\\Theta}(x)$')\n",
    "axs[1].plot(xx, -np.log(1-xx), linewidth=4, label=r'$J_{\\Theta}(x)$', c='orange')\n",
    "for i in [0,1]:\n",
    "    axs[i].spines['right'].set_color('none')\n",
    "    axs[i].spines['top'].set_color('none')\n",
    "    axs[i].spines['left'].set_position(('data',0))\n",
    "    axs[i].spines['bottom'].set_position(('data',0))\n",
    "    axs[i].xaxis.set_ticks_position('bottom')\n",
    "    axs[i].yaxis.set_ticks_position('left')\n",
    "    for label in axs[i].get_xticklabels() + axs[i].get_yticklabels():\n",
    "        label.set_fontsize(12)\n",
    "        label.set_bbox(dict(facecolor='white', edgecolor='None', alpha=0.65 ))\n",
    "    axs[i].legend(loc='upper center', prop={'size': 16})\n",
    "    axs[i].set_xlabel(r'$h_{\\Theta}$', fontsize=16)\n",
    "\n",
    "axs[0].text(0.2, 2.5, 'Falls y=1', fontsize=18)\n",
    "axs[1].text(0.2, 2.5, 'Falls y=0', fontsize=18)\n",
    "\n",
    "plt.savefig(\"LogistischeRegression09.png\",transparent=True, dpi=300)   \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die Kostenfunktion $\\hat{J}_{\\Theta}(x)$ ist konvex, daher können wir das Gradientenverfahren anwenden.\n",
    "\n",
    "Um die Fallunterscheidung für $y=0$ und $y=1$ zu eliminieren, können wir $\\hat{J}$ auch so formulieren:\n",
    "\n",
    "$$\n",
    "\\hat{J}_{\\Theta}(x)= -y\\log(h_{\\Theta}(x))- ( (1-y)\\log(1-h_{\\Theta}(x)))\n",
    "$$\n",
    "\n",
    "Um die endgültige Kostenfunktion zu erhalten, skalieren wir die Funktion noch durch die Anzahl der Datenpunkte und erhalten so\n",
    "\n",
    "$$\n",
    "J_{\\Theta}(x)= \\frac{1}{m} -y\\log(h_{\\Theta}(x))- ( (1-y)\\log(1-h_{\\Theta}(x)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Nun können wir das Gradientenverfahren anwenden.\n",
    "Skalieren wir wie gewohnt zuerst die Werte der Merkmale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = y.reshape(-1,1)\n",
    "\n",
    "scaling_factors = np.abs(XX[:,1:].max(axis=0)-XX[:,1:].min(axis=0))\n",
    "scaling_factors = np.concatenate([[1.0], scaling_factors])\n",
    "\n",
    "X_scaled = XX/scaling_factors\n",
    "X_scaled[0:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Im folgenden Code-Abschnitt werden die Modell-, Kosten- und Gradient-Descent Funktionen definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def h(X,theta):\n",
    "    #print(\"h -> X:\", np.shape(X), \"Theta:\", np.shape(theta))\n",
    "    return  1.0 / (1.0+np.exp(-(X@theta)))\n",
    "\n",
    "def J(X,theta,y):\n",
    "    yy = h(X,theta)\n",
    "    return -1/len(y) * (y.T@np.log(yy) + ((1-y).T@np.log(1-yy)))\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, iterationen):\n",
    "    kosten = []\n",
    "    for iter in range(iterationen):\n",
    "        costs = J(X, theta, y)\n",
    "        kosten.append(costs.item(0))\n",
    "        gradient = 1/len(y) * (X.T @ (sigma(X @ theta) - y))\n",
    "        theta = theta - (alpha * gradient)\n",
    "    return theta, kosten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Nun können wir die Parameter des Modells trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "theta0 = np.array([0,0]).reshape(2,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.3, random_state=0)\n",
    "X_train/=scaling_factors\n",
    "\n",
    "y = y.reshape(-1,1)\n",
    "theta_scaled, kosten = gradient_descent(X_train, y_train, theta0, 8, 1000)\n",
    "plt.plot(range(1,len(kosten)),kosten[1:], \"x-\")\n",
    "plt.xlabel(\"Epochen\", fontsize=14)\n",
    "plt.savefig(\"LogistischeRegression10.png\",transparent=True, dpi=300)\n",
    "plt.show()\n",
    "theta_gd = (theta_scaled.T/scaling_factors).T\n",
    "theta_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wir testen nun das trainierte Modell mit dem Testdatensatz und bestimmen die Vorhersagegenauigkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "theta_gd\n",
    "pred_gd = np.array([h(X_test,theta_gd)>=0.5])*1\n",
    "acc_gd=100-np.sum(np.abs(pred_gd-y_test))*100/len(y_test)\n",
    "print(\"Vorhersagegenauigkeit: %.2f%%\" % acc_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun wollen wir noch testen, zu welchen Ergebnissen die logistische Regression aus der Scikil-Learn Bibliothek kommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX[:,1:], y.reshape(-1), test_size=0.3, random_state=0)\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "logreg.intercept_, logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wie man sieht, sind die Parameter sehr ähnlich zu denen, die unser \"hand-kodiertes\" Gradientenverfahren liefert.\n",
    "Auch die Vorhersagedenauigkeit dieses Modells ist nahezu identisch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "acc_test=100-np.sum(np.abs(y_pred-y_test))*100/len(y_pred)\n",
    "\n",
    "print(\"Vorhersagegenauigkeit: %.2f%%\" % acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nun Plotten wir noch die edgültige Modellfunktion und tragen die Entscheidungsgrenze bei $h_{\\Theta}(x)=0.5$ ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "theta_gd = (theta_scaled.T/scaling_factors).T\n",
    "\n",
    "xx = np.linspace(-1,90,300).reshape(-1,1)\n",
    "xx = np.concatenate(((np.ones(len(xx))).reshape(-1,1), xx), 1)\n",
    "yy = h(xx,theta_gd)\n",
    "\n",
    "i = 0\n",
    "while(yy[i]<0.5): i+=1\n",
    "xx[:,1][i]\n",
    "\n",
    "plt.plot(xx[:,1], yy, c='orange', label=\"Modellfunktion\", linewidth=3)\n",
    "plt.scatter(X, y, label=\"Datenpunkte\")\n",
    "plt.xlabel(\"Lernstunden\", fontsize=14)\n",
    "plt.axvline(x=xx[:,1][i], c='r', ls=':', label=\"Entscheidungsgrenze\")\n",
    "plt.legend(loc='lower right', prop={'size': 12})\n",
    "plt.savefig(\"LogistischeRegression11.png\",transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entscheidungsgrenzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** **Verwenden Sie die oben beschrieben Techniken, um Entscheidungsgrenzen für einen zufällig erzeugten Datensatz zu berechnen.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# generating two-class dataset\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, center_box = (-5, 5))\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Teilen Sie den Datensatz auf (70% Training, 30% Test):\n",
    "- `X_train`: training dataset\n",
    "- `X_test`: test dataset\n",
    "- `y_train`: training labels\n",
    "- `y_test`: test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7c0c8cf666d3b5c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = [None]*4\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8818e07d53ed936b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert np.vstack((X_train, X_test)) in X and np.hstack((y_train, y_test)) in y\n",
    "assert y_train.size/y.size == 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Verwenden Sie die *sklearn*-Klasse `LogisticRegression` um ein Modell für den Datensatz zu bilden. Trainieren Sie das Modell mit den oben festgelegten Trainingsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12524a6611e42cf3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c2da293e49378c98",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert type(logreg) == LogisticRegression\n",
    "assert logreg.intercept_, 'Trainieren Sie das Modell mit den Daten!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir visualisieren nun den Datensatz um darzustellen, wie gut unser Modell klassifiziert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting decision regions\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.Spectral)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "plt.xlabel(r\"$\\Theta_0$\", fontsize=14)\n",
    "plt.ylabel(r\"$\\Theta_1$\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Testen Sie die Vorhersagegenauigkeit (*accuracy*) des Modells mit den Testdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f17cc003e3def17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = None\n",
    "acc_test = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"accuracy: %.2f%%\" % acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-35985385625486b8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert y_pred.shape == y_test.shape\n",
    "assert acc_test == 100-np.sum(np.abs(y_pred-y_test))*100/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = logreg.score(X_test, y_test)\n",
    "print('Test Accuracy Score', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen nun das Gradientenverfahren verwenden, um unser Modell selbst zu trainieren.\n",
    "\n",
    "**(d)** Initialisern Sie die Modellparameter $\\Theta$:\n",
    "\n",
    "*Hinweis:* Denken Sie daran, dass der Datensatz 2 Merkmale besitzt. Zusammen mit dem Bias-Parameter sollte $\\Theta$ also drei Eintäge haben. Die Dimension von $\\Theta$ ist demnach `(3,1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-04a8c2875082c89e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_train, y_test = y_train.reshape(-1,1), y_test.reshape(-1, 1)\n",
    "\n",
    "if X_train[0].size < 3:\n",
    "    X_train, X_test = np.concatenate((np.ones(y_train.shape) ,X_train), 1),np.concatenate((np.ones(y_test.shape) ,X_test), 1)\n",
    "\n",
    "\n",
    "theta0 = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "theta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3dee80b2f3db8eef",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert theta0.shape == (3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Verwenden Sie nun das Gradientenverfahren mit der oben definierten Methode `gradient_decent`:\n",
    "\n",
    "*Hinweis:* Experimentieren Sie mit verschiedenen Lernraten und Iterationen. Beobachten Sie die Resultate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-669dc2ff41610c40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "iterations = 40 #change it\n",
    "alpha = 1 #change it\n",
    "\n",
    "theta, costs = None, None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.plot(range(1,len(costs)) ,costs[1:], \"x-\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-abf0501a9f612c15",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert len(costs) == iterations\n",
    "assert theta.shape == theta0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting decision regions\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),np.arange(y_min, y_max, 0.01))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid1 = np.concatenate((np.ones((grid.shape[0], 1)), grid), 1)\n",
    "\n",
    "Z = h(grid1, theta) >= 0.5\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.Spectral)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "plt.xlabel(\"X0\", fontsize=14)\n",
    "plt.ylabel(\"X1\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Berechnen Sie die *saccuracy* des trainierten Moells und vergleichen Sie die Qualität mit dem vorherigen (*sklearn*) Modell.\n",
    "\n",
    "*Hinweis:* Berechnen Sie $h$ für die Datenpunkte im Test-Datensatz und weisen Sie allen Werten für $h$ $<0.5$ das Ergebnis $0$ und allen Werten für $h$ $\\geq 0.5$ das Ergebnis $1$ zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c08880a2930905af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = None\n",
    "acc_test = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"accuracy: %.2f%%\" % acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c15cc604a598429c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert y_pred.shape == y_test.shape\n",
    "assert acc_test == 100-np.sum(np.abs(y_pred-y_test))*100/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiklassen-Klassifikation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Die logistische Regression liefert uns Ergebnisse für binäre Zielvariable.\n",
    "Oftmals wollen wir aber mehr als 2 Klassen unterscheiden.\n",
    "\n",
    "Eine Möglichkeit, um Multiklassen-Klassifikation mit logistischen Regression umzusetzen ist die sogenannte *One-vs-all Klassifikation*.\n",
    "Dabei werden für `n` Klassen `n` separate, binäre Klassifikationsprobleme definiert, bei denen jeweils nur die betrachtete Klasse den Zielwert `1` zugeteilt bekommt, und für alle anderen Klassen der Zielwert `0` angenommen wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit-Learn unterstüzt Multiklassen-Klassifikation in der Klasse `LogisticRegression` über den Parameter `multi_class`.\n",
    "Setzt man : `multi_class=\"ovr\"` benutzt die führt die Funktion `fit` je eine logistische Regression für jedes Label nach dem *one-vs-all* (oder auch *one-vs-rest*, ovr) Prinzip aus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In der folgenden Code-Zelle erzeugen wir 3 Punktwolken.\n",
    "Alle Punkte einer \"Wolke\" sollen zu einer bestimmten Klasse gehören."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=100, centers=3, n_features=2, random_state=10)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "markers = {0:'o', 1:'x', 2:'^'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, marker=markers[key], color=colors[key])\n",
    "    \n",
    "plt.legend(loc='upper right', prop={'size': 12})\n",
    "plt.savefig(\"LogistischeRegression20.png\",transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Nun wenden wir ein logistisches Regressionsmodell auf die Datenbasis an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler',MinMaxScaler()),\n",
    "    ('model', LogisticRegression(solver='lbfgs',multi_class=\"ovr\"))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "\n",
    "params0 = pipeline.named_steps[\"model\"].intercept_\n",
    "paramsi  = pipeline.named_steps[\"model\"].coef_\n",
    "\n",
    "params0, paramsi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Der folgende Graph zeigt die Entscheidungsgrenzen für das Klassifikationsmodell.\n",
    "Alle Punkte innerhalb eines Bereiches werden der jeweiligen Klasse zugeordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "xx = np.linspace(X_train[:,0].min()-1, X_train[:,0].max()+1, 300)\n",
    "yy = np.linspace(X_train[:,1].min()-1, X_train[:,1].max()+1, 300)\n",
    "XX, YY = np.meshgrid(xx,yy)\n",
    "ZZ = pipeline.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "ZZ = ZZ.reshape(XX.shape)\n",
    "\n",
    "yyy = params0[0] + paramsi[0][0]*xx + paramsi[0][1]*yy\n",
    "\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "markers = {0:'o', 1:'x', 2:'^'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "#plt.pcolormesh(XX, YY, ZZ, cmap=plt.cm.Set3)\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, marker=markers[key], color=colors[key])\n",
    "    \n",
    "\n",
    "plt.contour(XX, YY, ZZ, cmap=plt.cm.Blues)\n",
    "plt.legend(loc='upper right', prop={'size': 12})\n",
    "plt.savefig(\"LogistischeRegression21.png\",transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
