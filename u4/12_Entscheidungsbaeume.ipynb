{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0418283f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2023\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entscheidungsbäume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Funktionsweise von Entscheidungsbäumen und dem CART Algorithmus zu demonstrieren, verwenden wir ein einfaches Beispiel mit nur sehr wenigen Datenpunkten.\n",
    "Bei den in der Code-Zelle unten angegebenen Wetterdaten `temperatur` und `niederschlag` handelt es sich um Monatsmittelwerte.\n",
    "Der Datensatz hat also nur 12 Punkte.\n",
    "\n",
    "Als Klassen assoziieren wir zu den Monaten je eine Jahreszeit.\n",
    "Vereinfachend zählen wir die Monate Dezember bis Februar zum Winter, März bis Mai zum Frühling, u.s.w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "temperatur = np.array([0.6,3.9,6.6,12.4,16.0,17.8,20.2,20.0,15.1,10.7,5.3,3.8])\n",
    "niederschlag = np.array([72,30,75,35,50,50,40,35,45,28,30,80])\n",
    "\n",
    "wetter_namen = ['Temperatur', 'Niederschlag']\n",
    "wetter_label_names = np.array(['Winter', 'Frühling', 'Sommer', 'Herbst'])\n",
    "wetter_label = np.array([0,0,1,1,1,2,2,2,3,3,3,0])\n",
    "wetter_tabelle = np.column_stack((temperatur,niederschlag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir aus dem Modul `sklearn.tree` die Klasse `DecisionTreeClassifier` verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "wetter_tree = DecisionTreeClassifier(max_depth=30, random_state=12)\n",
    "wetter_tree.fit(wetter_tabelle, wetter_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Visualisierung des Entscheidungsbaumes verwenden wird die Funktion `plot_tree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree \n",
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(wetter_tree, filled=True, feature_names=wetter_namen, class_names=wetter_label_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `plot_regions` stellt die Entscheidungsgrenzen eines Modells mit zwei Attributen und bis zu fünf Klassen dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_regions(X, model, labels, labelnames=None):\n",
    "    if labelnames is None:\n",
    "        labelnames = [i for i in np.unique(labels)]\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('gray', 'green', 'red', 'black', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(labels))])\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdBu)\n",
    "    for idx, cl in enumerate(np.unique(labels)):\n",
    "        plt.scatter(x=X[labels == cl, 0], y=X[labels == cl, 1],\n",
    "                    alpha=0.8, c=[cmap(idx)],\n",
    "                    marker=markers[idx], label=labelnames[cl])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir darstellen, wie die Monate nach den Kriterien *Temperatur* und *Niederschlag* den 4 Jahreszeiten zugeordnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_decision_regions(wetter_tabelle, wetter_label, classifier=wetter_tree, labelnames=wetter_label_names)\n",
    "plot_regions(wetter_tabelle, wetter_tree, wetter_label, wetter_label_names)\n",
    "\n",
    "plt.xlabel('Temperatur')\n",
    "plt.ylabel('Niederschlag')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was müssen Sie ändern, damit alle 12 Datenpunkte exakt eingeordnet werden? Was ist der Nachteil dieses Variante?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der CART Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine eigene Implementierung des CART Algorithmus fügen wir vorab die Datenpunkte und Labes zu einer Matrix zusammen.\n",
    "Dies vereinfacht das Teilen der Datensätze (*split*) etwas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack((wetter_tabelle,wetter_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die *Reiheit* einer Menge von Datenpunkten berechnen wir über das *Gini-Maß*.\n",
    "$$I_G=1-\\sum_{i=1}^np_i^2$$\n",
    "\n",
    "Die zugehörige Funktion `_gini` bestimmt zunächst die Menge der Labels.\n",
    "In unserer Matrix stehen die Labels in der letzten Spalte `data[:,-1]`.\n",
    "Da in einer Menge jeder Wert nur einmal enthalten sein kann sind in `c` nach der Operation je einmal die Werte 1-4 enthalten, mit denen die Jahreszeiten kodiert sind.\n",
    "\n",
    "Für jede Jahreszeit bestimmen wir die Anzahl an Datenpunkten die zu dieser Jahreszeit gehören.\n",
    "`d[:,-1]==cls` ist `True` für alle Zeilen der Matrix, die zu der Jahreszeit `cls` gehören.\n",
    "Mit der Operation `d[d[:,-1]==cls]` Rrduzieren wir die Matrix auf die Zeilen für die Jahreszeit `cls` und bestimmen dann mit `len` ihre Länge in Zeilen.\n",
    "Dieser Wert wird durch die Anzahl aller Datenpunkte `len(d)` geteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[tuple([data[:,-1]==1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gini(d):\n",
    "    if len(d)==0: return 1;\n",
    "    c = set(data[:,-1])\n",
    "    g=1\n",
    "    for cls in c:\n",
    "        p=len(d[d[:,-1]==cls])/len(d)\n",
    "        g -= p*p\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnen wir den *Gini-Wert* für die Ausgangsmenge, so erhalten wir das erwartete Resultat: $1-4\\left(\\frac{3}{12}\\right)^2=1-0.25=0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gini(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `_split` berechnet nun das Merkmal sowie die Bedingung an denen der Datensatz bestmöglich (d.h. mit maximalem Informationsgewinn) geteilt werden kann.\n",
    "$$𝐼𝐺_𝐼(𝐷,𝑓)=𝐼(𝐷)−\\sum_{𝐷_𝑖\\in𝑓(𝐷)}\\frac{|𝐷_𝑖|}{|𝐷|} 𝐼(𝐷_𝑖)$$\n",
    "\n",
    "Dazu läuft die Funktion in einer äußeren Schleife über alle Merkmale, also über alle Spaltennummern, bis auf die letzte (dort stehen die Labels).\n",
    "In jeder Spalte sortieren wir zunächst die Werte und bestimmen dann die möglichen Schnittpunkte.\n",
    "Diese Schnittpunkte liegen immer genau zwischen zwei aufeinander folgenden Werten.\n",
    "Mit `(last+i)/2` berechnen wir diesen Mittelwert und tragen in dann in die Liste `cuts` ein.\n",
    "\n",
    "Anschließend läuft die Funktion über alle möglichen Schnittpunkte und wertet jeweils die Kostenfunktion für den Informationsgewinn aus.\n",
    "\n",
    "Der global beste Wert und die das zugehörige Merkmal werden ermittelt und als Resultat zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split(data):\n",
    "    J_min=1               # 1 ist Maximum, sichere Inititalisierung\n",
    "    best_val=0\n",
    "    best_feature=0\n",
    "    N_data = len(data)\n",
    "    for feature in range(data.shape[1]-1):\n",
    "        cuts = []\n",
    "        f_sorted = np.sort(data[:,feature])\n",
    "        last = f_sorted[0]\n",
    "        for i in f_sorted[1:]:\n",
    "            cuts.append((last+i)/2)\n",
    "            last = i\n",
    "        for val in cuts:\n",
    "            true_set=data[data[:,feature]<=val]\n",
    "            false_set=data[data[:,feature]>val]\n",
    "            gini_t = _gini(true_set)\n",
    "            gini_f = _gini(false_set)\n",
    "            J = (gini_t*len(true_set)/N_data+gini_f*len(false_set)/N_data)\n",
    "            if J<=J_min:\n",
    "                J_min = J\n",
    "                best_val = val\n",
    "                best_feature=feature\n",
    "    \n",
    "    return best_feature,best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `_split` sollte nun in einer weiteren Funktion verwendet werden um den Entscheidungsbaum rekursiv aufzubauen.\n",
    "Für unser einfaches Beispiel ist es aber ebensogut möglich, die Schritte des Algorithmus \"per Hand\" durchzuspielen.\n",
    "\n",
    "Als ersten *Split* bestimmt die Funktion das Merkmal 0 (Temperatur) und den Wert 16.9.\n",
    "Die Kinder dieses Splits haben die *Gini-Werte* 0.67 (Temperatur kleiner oder gleich 16.9) und 0 (Temperatur größer 16.9).\n",
    "Damit wurde die Klasse der Sommermonate optimal abgedeckt.\n",
    "\n",
    "Da nur der linke Teilbaum einen *Gini-Wert* größer 0 hat, muss nur das linke Kind expandiert werden.\n",
    "Nun wird ebenfalls die Temperatur als Grundlage gewählt und die Wintermonate separiert.\n",
    "Im letzten Split werden die Frühjahr- und Herbstmonate über die Niederschlagsmenge getrennt.\n",
    "\n",
    "Der Entscheidungsbaum besitzt die selben Entscheidungsgrenzen, wie der von sklearn erzeugte.\n",
    "Allerdings besitzen die beiden Bäume eine andere Struktur, da die sklearn-Version zunächst die Wintermonate abspaltet.\n",
    "Dies hat ausschließlich mit der Reihenfolge der Berechnungen zu tun, das die *Gini-Werte* und Teilmengengrößen in beiden Fällen identisch sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature,best_val = _split(data)\n",
    "print(\"Split column\", best_feature, \"at\", best_val)\n",
    "\n",
    "left_set=data[data[:,best_feature]<best_val]\n",
    "right_set=data[data[:,best_feature]>=best_val]\n",
    "print(\"Gini left: %.2f | Gini right %.2f\" % (_gini(left_set), _gini(right_set)))\n",
    "\n",
    "best_feature,best_val = _split(left_set)\n",
    "print(\"Split column\", best_feature, \"at\", best_val)\n",
    "\n",
    "left_set1=left_set[left_set[:,best_feature]<best_val]\n",
    "right_set1=left_set[left_set[:,best_feature]>=best_val]\n",
    "print(\"Gini left: %.2f | Gini right %.2f\" % (_gini(left_set1), _gini(right_set1)))\n",
    "\n",
    "best_feature,best_val = _split(right_set1)\n",
    "print(\"Split column\", best_feature, \"at\", best_val)\n",
    "\n",
    "left_set2=right_set1[right_set1[:,best_feature]<best_val]\n",
    "right_set2=right_set1[right_set1[:,best_feature]>=best_val]\n",
    "print(\"Gini left: %.2f | Gini right %.2f\" % (_gini(left_set2), _gini(right_set2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreditvergabe mit Entscheidungsbäumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den folgenden Code-Zellen verwenden wir sklearn, um den aus dem letzten Arbeitsblatt bekannten Datensatz `kredit.csv` zu verarbeiten.\n",
    "Statt mit logistischer Regression wollen wir nun *gute* und *schlechte* Kredite über einen Entscheidungsbaum und später über *Random Forests* zu klassifizieren.\n",
    "\n",
    "Da die Schritte größtenteils selbsterklärend und aus vorherigen Beispielen bekannt sind, wird hier auf eine ausführliche Beschreibung verzichtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "url = \"https://github.com/fhswf/datasets/raw/main/kredit.csv\"\n",
    "dfile = \"./kredit.csv\"\n",
    "\n",
    "if not os.path.isfile(dfile):\n",
    "    urllib.request.urlretrieve(url, dfile)\n",
    "df = pd.read_csv(dfile)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import pydot\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "#model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "scr = model.score(X_test, y_test)\n",
    "print(\"Die Vorhersagegenauigkeit des Entscheidungsbaumes mit Tiefe 3 beträgt %.3f\" % scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(model, filled=True, feature_names=df.columns.values[1:], class_names=['schlecht','gut']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.3, random_state=0)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "scr = model.score(X_test, y_test)\n",
    "print(\"Die Vorhersagegenauigkeit des Entscheidungsbaumes beträgt %.3f\" % scr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun verwenden wir einen *Random Forrest* Klassifizierer mit einem Ensemble aus 100 Entscheidungsbaum-Instanzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forrest = RandomForestClassifier(n_estimators=100)\n",
    "forrest.fit(X_train,y_train)\n",
    "scr = forrest.score(X_test, y_test)\n",
    "print(\"Die Vorhersagegenauigkeit des Random Forests beträgt %.3f\" % scr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
