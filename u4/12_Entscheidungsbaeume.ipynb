{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0418283f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_S√ºdwestfalen_20xx_logo.svg/320px-Fachhochschule_S√ºdwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2023\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entscheidungsb√§ume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Funktionsweise von Entscheidungsb√§umen und dem CART Algorithmus zu demonstrieren, verwenden wir ein einfaches Beispiel mit nur sehr wenigen Datenpunkten.\n",
    "Bei den in der Code-Zelle unten angegebenen Wetterdaten `temperatur` und `niederschlag` handelt es sich um Monatsmittelwerte.\n",
    "Der Datensatz hat also nur 12 Punkte.\n",
    "\n",
    "Als Klassen assoziieren wir zu den Monaten je eine Jahreszeit.\n",
    "Vereinfachend z√§hlen wir die Monate Dezember bis Februar zum Winter, M√§rz bis Mai zum Fr√ºhling, u.s.w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "temperatur = np.array([0.6,3.9,6.6,12.4,16.0,17.8,20.2,20.0,15.1,10.7,5.3,3.8])\n",
    "niederschlag = np.array([72,30,75,35,50,50,40,35,45,28,30,80])\n",
    "\n",
    "wetter_namen = ['Temperatur', 'Niederschlag']\n",
    "wetter_label_names = np.array(['Winter', 'Fr√ºhling', 'Sommer', 'Herbst'])\n",
    "wetter_label = np.array([0,0,1,1,1,2,2,2,3,3,3,0])\n",
    "wetter_tabelle = np.column_stack((temperatur,niederschlag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir aus dem Modul `sklearn.tree` die Klasse `DecisionTreeClassifier` verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "wetter_tree = DecisionTreeClassifier(max_depth=30, random_state=12)\n",
    "wetter_tree.fit(wetter_tabelle, wetter_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√ºr die Visualisierung des Entscheidungsbaumes verwenden wird die Funktion `plot_tree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree \n",
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(wetter_tree, filled=True, feature_names=wetter_namen, class_names=wetter_label_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `plot_regions` stellt die Entscheidungsgrenzen eines Modells mit zwei Attributen und bis zu f√ºnf Klassen dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_regions(X, model, labels, labelnames=None):\n",
    "    if labelnames is None:\n",
    "        labelnames = [i for i in np.unique(labels)]\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('gray', 'green', 'red', 'black', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(labels))])\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),np.arange(y_min, y_max, 0.1))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdBu)\n",
    "    for idx, cl in enumerate(np.unique(labels)):\n",
    "        plt.scatter(x=X[labels == cl, 0], y=X[labels == cl, 1],\n",
    "                    alpha=0.8, c=[cmap(idx)],\n",
    "                    marker=markers[idx], label=labelnames[cl])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir darstellen, wie die Monate nach den Kriterien *Temperatur* und *Niederschlag* den 4 Jahreszeiten zugeordnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_decision_regions(wetter_tabelle, wetter_label, classifier=wetter_tree, labelnames=wetter_label_names)\n",
    "plot_regions(wetter_tabelle, wetter_tree, wetter_label, wetter_label_names)\n",
    "\n",
    "plt.xlabel('Temperatur')\n",
    "plt.ylabel('Niederschlag')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was m√ºssen Sie √§ndern, damit alle 12 Datenpunkte exakt eingeordnet werden? Was ist der Nachteil dieses Variante?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der CART Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√ºr eine eigene Implementierung des CART Algorithmus f√ºgen wir vorab die Datenpunkte und Labes zu einer Matrix zusammen.\n",
    "Dies vereinfacht das Teilen der Datens√§tze (*split*) etwas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack((wetter_tabelle,wetter_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die *Reiheit* einer Menge von Datenpunkten berechnen wir √ºber das *Gini-Ma√ü*.\n",
    "$$I_G=1-\\sum_{i=1}^np_i^2$$\n",
    "\n",
    "Die zugeh√∂rige Funktion `_gini` bestimmt zun√§chst die Menge der Labels.\n",
    "In unserer Matrix stehen die Labels in der letzten Spalte `data[:,-1]`.\n",
    "Da in einer Menge jeder Wert nur einmal enthalten sein kann sind in `c` nach der Operation je einmal die Werte 1-4 enthalten, mit denen die Jahreszeiten kodiert sind.\n",
    "\n",
    "F√ºr jede Jahreszeit bestimmen wir die Anzahl an Datenpunkten die zu dieser Jahreszeit geh√∂ren.\n",
    "`d[:,-1]==cls` ist `True` f√ºr alle Zeilen der Matrix, die zu der Jahreszeit `cls` geh√∂ren.\n",
    "Mit der Operation `d[d[:,-1]==cls]` Rrduzieren wir die Matrix auf die Zeilen f√ºr die Jahreszeit `cls` und bestimmen dann mit `len` ihre L√§nge in Zeilen.\n",
    "Dieser Wert wird durch die Anzahl aller Datenpunkte `len(d)` geteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[tuple([data[:,-1]==1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gini(d):\n",
    "    if len(d)==0: return 1;\n",
    "    c = set(data[:,-1])\n",
    "    g=1\n",
    "    for cls in c:\n",
    "        p=len(d[d[:,-1]==cls])/len(d)\n",
    "        g -= p*p\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnen wir den *Gini-Wert* f√ºr die Ausgangsmenge, so erhalten wir das erwartete Resultat: $1-4\\left(\\frac{3}{12}\\right)^2=1-0.25=0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gini(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `_split` berechnet nun das Merkmal sowie die Bedingung an denen der Datensatz bestm√∂glich (d.h. mit maximalem Informationsgewinn) geteilt werden kann.\n",
    "$$ùêºùê∫_ùêº(ùê∑,ùëì)=ùêº(ùê∑)‚àí\\sum_{ùê∑_ùëñ\\inùëì(ùê∑)}\\frac{|ùê∑_ùëñ|}{|ùê∑|} ùêº(ùê∑_ùëñ)$$\n",
    "\n",
    "Dazu l√§uft die Funktion in einer √§u√üeren Schleife √ºber alle Merkmale, also √ºber alle Spaltennummern, bis auf die letzte (dort stehen die Labels).\n",
    "In jeder Spalte sortieren wir zun√§chst die Werte und bestimmen dann die m√∂glichen Schnittpunkte.\n",
    "Diese Schnittpunkte liegen immer genau zwischen zwei aufeinander folgenden Werten.\n",
    "Mit `(last+i)/2` berechnen wir diesen Mittelwert und tragen in dann in die Liste `cuts` ein.\n",
    "\n",
    "Anschlie√üend l√§uft die Funktion √ºber alle m√∂glichen Schnittpunkte und wertet jeweils die Kostenfunktion f√ºr den Informationsgewinn aus.\n",
    "\n",
    "Der global beste Wert und die das zugeh√∂rige Merkmal werden ermittelt und als Resultat zur√ºckgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split(data):\n",
    "    J_min=1               # 1 ist Maximum, sichere Inititalisierung\n",
    "    best_val=0\n",
    "    best_feature=0\n",
    "    N_data = len(data)\n",
    "    for feature in range(data.shape[1]-1):\n",
    "        cuts = []\n",
    "        f_sorted = np.sort(data[:,feature])\n",
    "        last = f_sorted[0]\n",
    "        for i in f_sorted[1:]:\n",
    "            cuts.append((last+i)/2)\n",
    "            last = i\n",
    "        for val in cuts:\n",
    "            true_set=data[data[:,feature]<=val]\n",
    "            false_set=data[data[:,feature]>val]\n",
    "            gini_t = _gini(true_set)\n",
    "            gini_f = _gini(false_set)\n",
    "            J = (gini_t*len(true_set)/N_data+gini_f*len(false_set)/N_data)\n",
    "            if J<=J_min:\n",
    "                J_min = J\n",
    "                best_val = val\n",
    "                best_feature=feature\n",
    "    \n",
    "    return best_feature,best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `_split` sollte nun in einer weiteren Funktion verwendet werden um den Entscheidungsbaum rekursiv aufzubauen.\n",
    "F√ºr unser einfaches Beispiel ist es aber ebensogut m√∂glich, die Schritte des Algorithmus \"per Hand\" durchzuspielen.\n",
    "\n",
    "Als ersten *Split* bestimmt die Funktion das Merkmal 0 (Temperatur) und den Wert 16.9.\n",
    "Die Kinder dieses Splits haben die *Gini-Werte* 0.67 (Temperatur kleiner oder gleich 16.9) und 0 (Temperatur gr√∂√üer 16.9).\n",
    "Damit wurde die Klasse der Sommermonate optimal abgedeckt.\n",
    "\n",
    "Da nur der linke Teilbaum einen *Gini-Wert* gr√∂√üer 0 hat, muss nur das linke Kind expandiert werden.\n",
    "Nun wird ebenfalls die Temperatur als Grundlage gew√§hlt und die Wintermonate separiert.\n",
    "Im letzten Split werden die Fr√ºhjahr- und Herbstmonate √ºber die Niederschlagsmenge getrennt.\n",
    "\n",
    "Der Entscheidungsbaum besitzt die selben Entscheidungsgrenzen, wie der von sklearn erzeugte.\n",
    "Allerdings besitzen die beiden B√§ume eine andere Struktur, da die sklearn-Version zun√§chst die Wintermonate abspaltet.\n",
    "Dies hat ausschlie√ülich mit der Reihenfolge der Berechnungen zu tun, das die *Gini-Werte* und Teilmengengr√∂√üen in beiden F√§llen identisch sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature,best_val = _split(data)\n",
    "print(\"Split column\", best_feature, \"at\", best_val)\n",
    "\n",
    "left_set=data[data[:,best_feature]<best_val]\n",
    "right_set=data[data[:,best_feature]>=best_val]\n",
    "print(\"Gini left: %.2f | Gini right %.2f\" % (_gini(left_set), _gini(right_set)))\n",
    "\n",
    "best_feature,best_val = _split(left_set)\n",
    "print(\"Split column\", best_feature, \"at\", best_val)\n",
    "\n",
    "left_set1=left_set[left_set[:,best_feature]<best_val]\n",
    "right_set1=left_set[left_set[:,best_feature]>=best_val]\n",
    "print(\"Gini left: %.2f | Gini right %.2f\" % (_gini(left_set1), _gini(right_set1)))\n",
    "\n",
    "best_feature,best_val = _split(right_set1)\n",
    "print(\"Split column\", best_feature, \"at\", best_val)\n",
    "\n",
    "left_set2=right_set1[right_set1[:,best_feature]<best_val]\n",
    "right_set2=right_set1[right_set1[:,best_feature]>=best_val]\n",
    "print(\"Gini left: %.2f | Gini right %.2f\" % (_gini(left_set2), _gini(right_set2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreditvergabe mit Entscheidungsb√§umen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den folgenden Code-Zellen verwenden wir sklearn, um den aus dem letzten Arbeitsblatt bekannten Datensatz `kredit.csv` zu verarbeiten.\n",
    "Statt mit logistischer Regression wollen wir nun *gute* und *schlechte* Kredite √ºber einen Entscheidungsbaum und sp√§ter √ºber *Random Forests* zu klassifizieren.\n",
    "\n",
    "Da die Schritte gr√∂√ütenteils selbsterkl√§rend und aus vorherigen Beispielen bekannt sind, wird hier auf eine ausf√ºhrliche Beschreibung verzichtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "url = \"https://github.com/fhswf/datasets/raw/main/kredit.csv\"\n",
    "dfile = \"./kredit.csv\"\n",
    "\n",
    "if not os.path.isfile(dfile):\n",
    "    urllib.request.urlretrieve(url, dfile)\n",
    "df = pd.read_csv(dfile)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import pydot\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "#model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "scr = model.score(X_test, y_test)\n",
    "print(\"Die Vorhersagegenauigkeit des Entscheidungsbaumes mit Tiefe 3 betr√§gt %.3f\" % scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(model, filled=True, feature_names=df.columns.values[1:], class_names=['schlecht','gut']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.3, random_state=0)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "scr = model.score(X_test, y_test)\n",
    "print(\"Die Vorhersagegenauigkeit des Entscheidungsbaumes betr√§gt %.3f\" % scr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun verwenden wir einen *Random Forrest* Klassifizierer mit einem Ensemble aus 100 Entscheidungsbaum-Instanzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forrest = RandomForestClassifier(n_estimators=100)\n",
    "forrest.fit(X_train,y_train)\n",
    "scr = forrest.score(X_test, y_test)\n",
    "print(\"Die Vorhersagegenauigkeit des Random Forests betr√§gt %.3f\" % scr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
