{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d55a1c6",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2023\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt betrachten wir ein Verfahren, dass dem unüberwachten Lernen (*unsupervised learning*) zuzuordnen ist, nämlich der Hauptkomponentenzerlegung (engl. *Principal Component Analysis*, PCA).\n",
    "PCA ein Algorithmus zur Reduzierung der Dimensionalität.\n",
    "Das bedeutet, dass PCA Linearkombinationen der Merkmale berechnet, die den Datensatz mit möglichtst geringem Informationsverlust beschreiben.\n",
    "Wir haben die Hauptkomponentenzerlegung bereits im Arbeitsblatt zur Datenvorverarbeitung kennen gelernt und auch verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen Eindruck von der Funktionsweise der PCA zu bekommen, wollen wir uns einen zufällig generierten Datensatz anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "nfeatures = 2\n",
    "nsize = 200\n",
    "X = np.dot(np.random.rand(nfeatures, 2), np.random.randn(2, nsize)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man sieht in dem Streudiagramm, dass es im vorliegenden Datensatz eine Wechselbeziehung zwischen dem Merkmal *x* und dem Merkmal *y* gibt.\n",
    "In vorherigen Aufgaben haben eine solche Wechselbeziehung bereits ausgenutzt, z.B. um mittels der Linearen Regression die Variable $y$ durch die Variable *x* zu beschreiben.\n",
    "Wir haben gesehen, dass man *y* durch ein lineares Modell annähern kann.\n",
    "\n",
    "Bei der Hauptkomponentenzerlegung wird die Wechselbeziehung zwischen Variablen ebenfalls ausgenutzt, aber nicht mit dem Ziel, ein Merkmal zu beschreiben.\n",
    "Vielmehr werden die Wechselbeziehungen in Form von Linearkombinantionen der Merkmale (den sog. *Hauptkomponenten*) *gelernt*, um den kompletten Datensatz mit weniger Informationen beschreiben zu können.\n",
    "\n",
    "Mit *Scikit-Learn* können wir die Hauptkomponenten wie folgt berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `fit`-Funktion lernt die Hauptkomponenten des Datensatzes\n",
    "Im trainierten PCA-Modell sind die Eigenvektoren der Kovarianzmatrix unter dem Attribut `components_` zugreifbar, die Eigenvalues werden als `explained_variance_` abgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kann diese Parameter auch leicht selbst berechnen, indem man die Funktionen zur Bestimmung der Kovarianzmatrix (`np.cov`) und der Singulärwertzerlegung (`np.linalg.svd`) von NumPy verwendet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kovmat = np.cov(X.T)\n",
    "u,s,v = np.linalg.svd(kovmat)\n",
    "print(\"Eigenvektoren :\", v)\n",
    "print(\"Eigenwerte:\", s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Hauptkomponenten verlaufen in Richtung der Eigenvektoren, die Eigenwerte geben das Maß der Streuung der Datenpunkte entlang dieser Hauptkomponente an.\n",
    "Im folgenden Diagramm zeigen die Vektorpfeile den Verlauf der Hauptkomponenten an. Die Länge der Pfeile spiegelt die Varianz der Hauptkomponente wieder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "\n",
    "ursprung = (0,0)\n",
    "pc = []\n",
    "colors = ['r', 'b', 'y', 'g']\n",
    "for i in range(len(v)):\n",
    "    pc.append((v[i][0]*np.sqrt(s[i]), v[i][1]*np.sqrt(s[i])))\n",
    "    plt.quiver(*ursprung, *pc[i], color=colors[i], scale=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA zur Reduktion der Dimensionalität\n",
    "\n",
    "Bisher haben das PCA-Verfahren genutzt, um die Hauptkomponenten des Datensatzes zu bestimmen.\n",
    "Im obigen Beispiel hatten wir zwei Variablen und ebenfalls zwei Hauptkomonenten.\n",
    "Unser ursprünglicher Datensatz hat sich dadurch nicht verändert.\n",
    "\n",
    "Um Anzahl der Merkmale reduzieren, können wir aber die Ergebnisse der PCA verwenden undem wir die Werte entlang einer oder mehrerer der kleinsten Hauptkomponenten Null setzen.\n",
    "Geometrisch gesehen, entspricht dies einer Projektion der Datenpunkte auf die dominierenden Hauptkomponenten.\n",
    "Mit jedem \"weglassen\" einer Hauptkomponente verliert man zwar einen Teil der Information aus dem Datensatz (mathematisch gesehen entspricht der Informationsgehalt der *Varianz*). Da es sich aber um die Hauptkomponente mit der Jeweils geringsten Varianz handelt, ins der Informationsverlust minimal.\n",
    "\n",
    "Betrachten wir nun, was mit unserem obigen Beispiel passiert, wenn wir den Datensatz von zwei auf eine Variable reduzieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"Ursprüngliche Größe:   \", X.shape)\n",
    "print(\"UTransformierte Größe:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der transformierte Datensatz ist nur noch halb so groß.\n",
    "Um zu sehen, wie die Daten projiziert wurden, kann man die inverse Transformation durchführen und die Daten in so wieder auf den ursprünglichen 2-dimensionalen Raum abbilden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], c='b', alpha=0.8)\n",
    "plt.axis('equal');\n",
    "print(\"Der Datensatz enthält %.1f%c der ursprünglichen Information (Varianz)\" % (100*pca.explained_variance_ratio_[0], '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die hellen Punkte sind die Originaldaten, die dunklen Punkte die transformierten Daten.\n",
    "Man sieht, dass im transformierten Datensatz alle Punkte entlang einer Geraden laufen, auf die die ursprünglichen Punkte projiziert wurden.\n",
    "\n",
    "Dieser Datensatz mit reduzierter Dimension ist in gewisser Hinsicht *gut genug*, um die wichtigsten Beziehungen zwischen den Punkten zu erfassen: Trotz der Reduzierung der Dimension und damit der Datenmenge um 50% bleibt die Varianz zu fast 95% erhalten.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA zur Visualisierung\n",
    "\n",
    "Das Einsatzspektrum von PCA zeigt sich besonders bei Datensätzen mit seer hoher Dimensionen.\n",
    "Im Folgenden wollen wir die PCA-Methode auf einen Bild-Datensatz mit weit mehr als 2 Merkmalen anwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(f\"Ziffern Datensatz mit {digits.data.shape[0]} Datenpunkten und {digits.data.shape[1]} Merkmalen\")\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeder Datenpunkt besteht aus einem grob aufgelösten Graustufenbild von 8-mal-8 Pixeln einer handgeschriebenen Ziffer.\n",
    "Jedes Pixel wird als Merkmal gedeutet.\n",
    "In Summe haben wir es hier also mit 64 unterschiedlichen Variablen zu tun.\n",
    "\n",
    "Wir wollen nun den Datensatz auf zwei Hauptkomponenten reduzieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)\n",
    "print(\"Der Datensatz enthält noch %.1f%c der ursprünglichen Information (Varianz)\"\n",
    "      % (100*sum(pca.explained_variance_ratio_), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir nun unsere Ziffern-Bilder durch 2 Merkmale beschreiben, können wir die Datenpunkte in der Ebene plotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('viridis', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[0], pca.explained_variance_ratio_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was bedeutet diese VEreinfachung?\n",
    "Wir sind nun von 64 Pixeln auf 2 Merkmale herunter gekommen, haben unseren Datensatz also um den Faktor 32 verkleinert.\n",
    "Damit erhalten wir nicht nur weniger Daten, auf eine Modellfunktion lässt sich auf weniger Merkmale hin deutlich leichter trainieren.\n",
    "\n",
    "Allerdings müssen wir uns die Frage stellen, ob 2 Merkmale hier noch ausreichend für gute Vorhersagen sind.\n",
    "Wir sehen zwar in der Abbildung, dass die Punktwolken für die einzelnen Ziffern gebündelt auftreten.\n",
    "Aber die Wolken reichen doch deutlich ineinander, sodass es sehr schwierig sein dürfte, klare Entscheidungsgrenzen zu finden.\n",
    "\n",
    "Auch die statistischen Kennwerte sprechen dagegen, nur zwei Dimensionen zu verwenden.\n",
    "Durch die übrig gebliebenen Merkmale, werden nur 28.5% der ursprünglichen Varianz abgedeckt.\n",
    "Diese Reduktion der Information ist hier vermutlich zu hoch, um einem Klassifizierungsalgorithmus noch eine gute Datenbasis zu liefern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Anzahl der Komponenten bestimmen\n",
    "\n",
    "Um eine passende Anzahl von Hauptkomponenten festzulegen, kann uns wieder die allgmeine PCA helfen.\n",
    "Hierbei werden ja alle Hauptkomponenten gefunden und nach ihrer Varianz absteigend sortiert.\n",
    "Wir betrachten nun die kummulative, relative Varianz.\n",
    "Beginnend mit der *wichtigsten* Hauptkomponente summieren wir den Anteil, den die Komponenten an der gesamten Varianz des Datensatzes abbilden, auf.\n",
    "So entsteht die unten abgebildete Kurve.\n",
    "Wir sehen, wie viel uns das hinzunehmen einer Weiteren Komponte zum Modell *bringt*.\n",
    "Um z.B. 90% der Varianz zu erhalten, benötig man ca. 20 Hauptkomponenten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA zur Rauschunterdrückung\n",
    "\n",
    "Die HAuptkomponentenzerlegung kann auch verwendet werden, um verrauschte Daten aufzubereiten.\n",
    "Die Idee ist wie folgt:\n",
    "Komponenten mit einer Varianz die höher als die des Rauschens ist, sollten durch das Rauschen kaum beeinträchtigt sein.\n",
    "Wenn wir also nur diese wichtigen Hauptkomponenten werwenden, sollte das Rauschen nach dem wiederherstellen der Daten zu großen Teilen eliminiert sein.\n",
    "\n",
    "Wir nehmen wieder den Ziffern Datensatz und legen über die Bilder ein künstliches Rauschen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 16))\n",
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun fügen wir den Bilddaten ein Gauss'sches Rauschen hinzu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "noisy = np.random.normal(digits.data, 4)\n",
    "plot_digits(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man sieht, dass die Bilder nun deutlich *verpixelt* sind.\n",
    "Auf diesen Daten, berechnen wir nun ein PCA Modell, dass noch 50% der Varianz erhalten soll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.50).fit(noisy)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die 50%  der Varianz zu erreichen, müssen 12 Hauptkomponenten verwendet werden.\n",
    "Wir berechnen nun diese Hauptkomponenten und stellen mit diesen die Bilddaten über eine inverse Transformation wieder her:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.transform(noisy)\n",
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie man sieht, ist das Ruaschen deutlich unterdrückt.\n",
    "Die Bilder wirken zwar etwas *weich gezeichnet*, aber die Konturen der ursprünglichen Ziffern sind deutlicher zu erkennen, als bei den verpixelten Bildern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quelle:**\n",
    "\n",
    "[1] Jake VanderPlas, [*Python Data Science Handbook*](https://jakevdp.github.io/PythonDataScienceHandbook), O'Reilly, 2016"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
