{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e8d22a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2023\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import *\n",
    "from ipywidgets.widgets import IntSlider\n",
    "from ipywidgets import interact\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation mit Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher haben wir in überwachten ML-Agorithmen Daten verwendet, um zu lernen, zu welchen Klassen die Daten gehören:\n",
    "- Die **logistische Regression** lernt Abhängigkeiten zwischen Eingabedaten und Zielen\n",
    "- **Entscheidungsbäume** verwenden eine Hirarchiestruktur, um die Auswirkungen verschiedener Attribute auf das Ziel abzuwägen\n",
    "\n",
    "In beiden Verfahren werden Entscheidungsgrenzen gelernt, mit denen sich die Punkte des Trainingsdatensatzes möglichst gut separieren lassen.\n",
    "Der konkrete Verlauf der Entscheidungsgrenzen wird dabei nicht nicht berücksichtig.\n",
    "D.h., wenn es mehrere Entscheidungsgrenzen gibt, die die Punkte bestmöglich klassifizieren, liefern die o.g. Verfahren nicht unbedingt die beste Entscheidungsgrenze.\n",
    "\n",
    "Aber was bedeutet überhaupt *beste* Entscheidungsgrenze?\n",
    "Man kann sich leicht vorstellen, dass eine Entscheidungsgrenze umso besser ist, je mehr sie durch die *Mitte* der Raumes verläuft, der zwischen den *Punktwolken* der einzelnen Klassen liegt.\n",
    "Wenn die Entscheidungsgrenze so eingezogen wird, ist der Abstand zwischen den *äußeren* Punkten der Trainingsdaten und der Entscheidungsgrenze maximal groß.\n",
    "Mathematisch gesprochen erzeugt ein Klassifikator, der nach diesem Prinzip funktioniert, Hyperebenen mit größtmöglicher geometrischer Marge.\n",
    "Man spricht daher im Englischen auch von einem *Maximal Margin Classifier*.\n",
    "\n",
    "Die *Support Vector Machine* (SVM) ist eine Form der *Maximal Margin Classifier*.\n",
    "Mit SVMs lassen sich sowohl lineare als auch nichtlineare Klassifikationsaufgaben lösen.\n",
    "Ebenfalls können SVMs zur Regression oder Ausreißer-Erkennung eingesetzt werden.\n",
    "SVMs zählen zur Klasse der Instanz-basierten Lernverfahren und eignen sich besonders zur Klassifikation komplexer Datensätze (viele Merkmale) mit kleiner oder mittlerer Größe (moderate Anzahl von Datenpunkten).\n",
    "Im Gegensatz zu statistischen Ansätzen (wie etwa der der logistischen Regression) basiert die SVM auf den geometrischen Eigenschaften der Daten.\n",
    "Dabei ist die Bedeutung der Variablen bei der SVM weniger relevant, weswegen dieses Modell gut mit unstrukturierten und halbstrukturierten Daten wie Texten und Bildern funktioniert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "n_data = 50\n",
    "\n",
    "# generating two-class dataset\n",
    "X, y = make_blobs(n_samples=n_data, centers=n_classes, random_state=1, center_box=(0, 10))\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "y_col = np.array(['red' if i == 0 else 'lime' for i in y])\n",
    "\n",
    "# plotting the data according to class\n",
    "for i, s, m in [(0, 100, 'o'), (1, 80, 's')]:\n",
    "    plt.scatter(X[y==i,0], X[y==i,1], c=y_col[y==i], s=s, marker = m, edgecolors='black')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die beiden Datencluster sind eindeutig linear trennbar.\n",
    "Dies ist nicht immer der Fall, für den Moment gehen wir aber dvon aus, dass dies für unseren Datensatz gilt.\n",
    "Wo können wir die Grenze ziehen?\n",
    "In der folgenden Abbildung ziehen wir mehrere möglich Hyperebenen zwischen die Punktewolken ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xline = np.linspace(0,1)\n",
    "\n",
    "# plotting the data according to class\n",
    "for i, s, m, label in [(0, 100, 'o', 'Negative'), (1, 80, 's', 'Positive')]:\n",
    "    plt.scatter(X[y==i,0], X[y==i,1], c=y_col[y==i], s=s, marker = m, edgecolors='black', label=label)\n",
    "\n",
    "# plotting potential hyperparameters\n",
    "for m, b in [(-1, 1), (0, 0.45)]:\n",
    "    plt.plot(xline, m * xline + b)\n",
    "plt.plot(xline*0+0.55, xline)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jede der Hyperebenen separiert die *Positiven* und *Negativen* Datenpunkte optimal.\n",
    "Aber nicht alle Hyperebenen sind **optimal generalisierend**.\n",
    "Bei der grünen und orongenen Hyperebene ibt es *Bereiche*, die zwar nach an den Punkten einer Klasse liegen, aber dem Bereich der anderen Klasse zugeordnet sind.\n",
    "\n",
    "Die blaue Hyperebene maximiert auch die Bereiche *um* die Punktwolken, sie schafft also die breiteste Marge (eng. *Margin*) zwischen den Punkten *am Rand* und der trennenden Hyperebene.\n",
    "Daher ist sie objektiv besser als die beiden anderen eingezeichneten Möglichkeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xline = np.linspace(0, 1)\n",
    "yline = -1.5*xline+1.3\n",
    "delta = 0.35\n",
    "\n",
    "# plotting the data according to class\n",
    "for i, s, m, label in [(0, 100, 'o', 'Negative'), (1, 80, 's', 'Positive')]:\n",
    "    plt.scatter(X[y==i,0], X[y==i,1], c=y_col[y==i], s=s, marker = m, edgecolors='black', label=label)\n",
    "\n",
    "# plotting decision boundry\n",
    "for s, m in [(1, 'k--'),(0, 'k'),(-1, 'k--')]:\n",
    "    plt.plot(xline, yline +delta*s, 'k--', alpha = 0.8)\n",
    "\n",
    "# Annotationen\n",
    "plt.text(0.26, 0.64, s='Trennende\\nHyperebene\\n',fontsize= 13, rotation = -50)\n",
    "plt.annotate(text='', xy=(xline[30], yline[30]), xytext=(xline[36],yline[36]+delta), arrowprops=dict(arrowstyle='<->'))\n",
    "plt.text(0.7, 0.3, s='Marge',fontsize= 13, rotation = -45)\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.axis('off')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM Klassifizierer** finden Hyperebenen mit maximaler Marge, sind also als *Maximal Margin Classifier* einzustufen.\n",
    "Um eine *Support Vector Machine* mit `sklearn` zu erstellen, benötigen wir ein Objekt vom Typ `SVM`. Dieses Modell trainieren wir auf den entsprechenden Datensatz mit der `fit`-Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear', C=1e5).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir die gefunden Hyperebenen darstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting data\n",
    "for i, s, m, label in [(0, 100, 'o', 'Negative'), (1, 80, 's', 'Positive')]:\n",
    "    plt.scatter(X[y==i,0], X[y==i,1], c=y_col[y==i], s=s, marker = m, edgecolors='black', label=label)\n",
    "\n",
    "# support vectors\n",
    "plt.scatter(model.support_vectors_[:, 0],\n",
    "           model.support_vectors_[:, 1],\n",
    "           s=400, edgecolors='black', facecolors='none')\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(0, 1, 30)\n",
    "yy = np.linspace(0, 1, 30)\n",
    "yy, xx = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "P = model.decision_function(xy).reshape(xx.shape)\n",
    "\n",
    "# plotting decision boundry\n",
    "plt.contour(xx, yy, P, colors='k', levels=[-1, 0, 1],\n",
    "            alpha=0.8, linestyles=['--', '-', '--'])\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wo die Hyperebene mit der maximalen Marge liegt, wird von den Punkten am äußeren *Rand* der Punktwolken bestimmt (in der Abbildung zu erkennen durch die schwarze Umrandung).\n",
    "Diese Punkte nennt man auch **Support Vektoren**.\n",
    "Wie sich leicht erkennen lässt, haben diese Punkte eine herausragende Bedeutung für SVMs.\n",
    "Im Folgenden wollen daher betrachten, wie man diese Punkte identifizieren kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir von einem linearen Modell ausgehen, lässt sich unsere Hyperebene im $n$-dimensionalen Raum mit der Gleichung\n",
    "$$\\langle\\textbf{w},\\textbf{x}\\rangle+b=0$$\n",
    "beschreiben.\n",
    "\n",
    "Dabei ist $\\langle \\textbf{w}, \\textbf{x} \\rangle$ das Standardskalarprodukt des Paramter-Vektors $\\mathbf{w}$ und des Eingabe-Vektors $\\mathbf{x}$. Wenn wir davon ausgehen, dass $\\textbf{w}$ und $\\textbf{x}$ Spaltenvektoren sind, erhalten wir das Skalarprodukt, wenn wir $\\textbf{w}$ durch Transponieren zum Zeilenvektor machen und dann die generelle Matrizenmultiplkation anwenden. Die obige Gleichung ist also identisch mit der Form $ \\textbf{w}^T\\textbf{x}+b$\n",
    "\n",
    "\n",
    "\n",
    "Eine solche Hypereben trent den $n$-dimensionalen Raum in zwei Halbräume.\n",
    "Auf welcher Seite, also in welchem Halbraum ein Punkt mit dem Ortsvektor $\\textbf{x}_i$ liegt, erkennt man durch einsetzen in die Gleichung. Aller Punkte $\\textbf{x}_i$, mit\n",
    "- $\\textbf{w}^T\\textbf{x}_i+b<0$ liegen in der ersten Halbebene\n",
    "- $\\textbf{w}^T\\textbf{x}_i+b=0$ liegen auf der Ebene\n",
    "- $\\textbf{w}^T\\textbf{x}_i+b>0$ liegen in der zweiten Halbebene\n",
    "\n",
    "Nehmen wir an, dass die Punkte in der ersten Halbebene die Label $y_i=-1$ tragen und die Punkte auf der zweiten Halbebene die Label $y_i=1$, so gilt:\n",
    " \n",
    "$$y_i(\\textbf{w}^T\\textbf{x}_i+b)>0$$\n",
    "\n",
    "Wenn wir also die Vorzeicheinfunktion verwenden erhalten wir auf diesem Weg einen Klassifizierer, der die Punkte im Raum in die Klassen $-1$ (Negative) und $1$ (Positive) aufteilt:\n",
    "$$ \\hat{y}=sign(\\textbf{w}^T\\textbf{x}_i+b)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten\n",
    "for i, s, m, label in [(0, 100, 'o', 'Negative'), (1, 80, 's', 'Positive')]:\n",
    "    plt.scatter(X[y==i,0], X[y==i,1], c=y_col[y==i], s=s, marker = m, edgecolors='black', label=label, alpha=0.2)\n",
    "\n",
    "# Entscheidungsgrenze\n",
    "plt.contour(xx, yy, P, colors='k', levels= 0,\n",
    "            alpha=0.8, linestyles='-')\n",
    "\n",
    "# Normalenvektor w und Bias b\n",
    "for xy, xyt, st, lxy, l in [((0,0), (0.42,0.68), '<->', (0.15, 0.4), '$b$'),\n",
    "#                           ((0.52, 0.52), (0.57,0.6), '<|-', (0.5, 0.6), '$\\overrightarrow w$'),\n",
    "                            ((0.52, 0.52), (0.905,1.135), '<|-', (0.5, 0.6), '$\\overrightarrow w$'),\n",
    "                           ((0.52, 0.52), X[28], '<|-', (0.62, 0.48), '$\\overrightarrow x_{neg}$')]:\n",
    "    plt.annotate(text='', xy=xy, xytext=xyt, arrowprops=dict(arrowstyle=st))\n",
    "    plt.text(lxy[0], lxy[1], s=l,fontsize= 14)\n",
    "\n",
    "\n",
    "plt.axis('on')\n",
    "plt.xlim([0, 1.3])\n",
    "plt.ylim([0, 1.3])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da sich es sich im Folgenden mit einer *Größer-Bedingung* nicht so einfach rechnen lässt, wandeln wir die Abstandsgleichung so um, dass für alle Punkte ein Mindestabstand zur trennenden Hyperebene gefordert wird.\n",
    "Diesen Abstand können wir auf $1$ setzen, um die mathematischen Voraussetzungen zu vereinfachen.\n",
    "Hierbei handelt es sich lediglich um eine Art Normierung, denn jeder andere Abstandswert würde sich allein durch Skalierung der Parameter $\\textbf{w}$ und $b$ erreichen lassen.\n",
    "Wir können unsere Forderung also umschreiben zu:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\textbf{w}^T\\textbf{x}_i + b \\geq +1, & y_i = +1 (Positive)\\\\\n",
    "    \\textbf{w}^T\\textbf{x}_i + b \\leq -1, & y_i = -1 (Negative)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "oder zusammengefasst als:\n",
    "\n",
    "$$y_i(\\textbf{w}^T\\textbf{x}_i+b)\\geq1$$\n",
    "\n",
    "Mit dieser Forderung erhält man nun 2 weitere Hyperebenen $H_1$ und $H_2$, die parallel zur Entscheidungsgrenze stehen:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "H_1 = &\\textbf{w}^T\\textbf{x}_i + b = +1, & \\text{Hyperebene in der Klasse mit } y_i = +1 (Positive)\\\\\n",
    "H_2 = &\\textbf{w}^T\\textbf{x}_i + b = -1, & \\text{Hyperebene in der Klasse mit } y_i = -1 (Negative)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting data\n",
    "for i, s, m, label in [(0, 100, 'o', 'Negative'), (1, 80, 's', 'Positive')]:\n",
    "    plt.scatter(X[y==i,0], X[y==i,1], c=y_col[y==i], s=s, marker = m, edgecolors='black', label=label, alpha=0.2)\n",
    "\n",
    "# plotting decision boundry\n",
    "plt.contour(xx, yy, P, colors='k', levels= [-1 ,0, 1],\n",
    "            alpha=0.8, linestyles=['--', '-', '--'])\n",
    "\n",
    "# Plotting wieght and bias\n",
    "for xy, xyt, st, lxy, l in [((0,0), (0.42,0.68), '<->', (0.15, 0.4), '$b$'),\n",
    "#                           ((0.52, 0.52), (0.57,0.6), '<|-', (0.5, 0.6), '$\\overrightarrow w$'),\n",
    "                           ((0.52, 0.52), (0.905,1.135), '<|-', (0.5, 0.6), '$\\overrightarrow w$'),\n",
    "                           ((0.52, 0.52), X[28], '<|-', (0.62, 0.48), '$\\overrightarrow x_{neg}$')]:\n",
    "    plt.annotate(text='', xy=xy, xytext=xyt, arrowprops=dict(arrowstyle=st))\n",
    "    plt.text(lxy[0], lxy[1], s=l,fontsize= 14)\n",
    "\n",
    "plt.axis('on')\n",
    "plt.xlim([0, 1.3])\n",
    "plt.ylim([0, 1.3])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser Midnestabstand zur Entscheidungsgrenze impliziert, dass kein Punkt innerhalb der Marge zwischen $H_1$ und $H_2$ liegt.\n",
    "Die meisten der Datenpunkte liegen abseits dieser Hyperebenen innerhalb der Halbräume.\n",
    "Die Datenpunkt, die genau auf den Hyperebenen $H_1$ und $H_2$ liegen, nennt man **Support Vektoren**.\n",
    "Der Abstand $\\delta$  der Hyperplanes $H_1$ und $H_2$ von der Entscheidungsgrenze kann man durch den Abstand der Support Vektoren ausdrücken:\n",
    "$$2\\delta=(\\textbf{x}_{pos}-\\textbf{x}_{neg})\\cdot\\frac{\\textbf{w}}{|\\textbf{w}|}$$\n",
    "$$2\\delta=\\frac{\\textbf{w}^T \\textbf{x}_{pos} - \\textbf{w}^T \\textbf{x}_{neg}}{|\\textbf{w}|}$$\n",
    "$$2\\delta=\\frac{(b+1)-(b-1)}{|\\textbf{w}|}$$\n",
    "$$2\\delta=\\frac{2}{|\\textbf{w}|} \\implies \\delta=\\frac{1}{|\\textbf{w}|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting data\n",
    "for i, s, m, label in [(0, 100, 'o', 'Negative'), (1, 80, 's', 'Positive')]:\n",
    "    plt.scatter(X[y==i,0], X[y==i,1], c=y_col[y==i], s=s, marker = m, edgecolors='black', label=label, alpha=0.2)\n",
    "for i, m, c in [(0, 'o', 'red'),(1, 's', 'lime')]:\n",
    "    plt.scatter(model.support_vectors_[i, 0],\n",
    "               model.support_vectors_[i, 1],marker = m,\n",
    "               s=200, edgecolors='black', facecolors=c)\n",
    "\n",
    "\n",
    "# plotting decision boundry\n",
    "plt.contour(xx, yy, P, colors='k', levels= [-1 ,0, 1],\n",
    "            alpha=0.8, linestyles=['--', '-', '--'])\n",
    "\n",
    "# annotation\n",
    "for xy, i, lxy in [((0.47,0.61), 1, (0.35, 0.55)),\n",
    "                    ((0.55,0.46), 0, (0.62, 0.5))]:\n",
    "    plt.annotate(text='', xy=xy, xytext=model.support_vectors_[i], arrowprops=dict(arrowstyle='<|-|>'))\n",
    "    plt.text(lxy[0], lxy[1], s='$\\delta$',fontsize= 14, rotation=45)\n",
    "\n",
    "plt.xlim(0.1,0.9)\n",
    "plt.ylim(0.1,0.9)\n",
    "plt.axis('off')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um $\\delta=\\frac{1}{|\\textbf{w}|}$ zu maximieren, muss $|\\textbf{w}|$ minimiert werden. \n",
    "Auch hier wenden wir einen kleinen Trick an.\n",
    "Da sich quadratische Funktionen leichter optimieren lassen, bestimmen wir äquivalent das Minimum von $\\frac{1}{2}|\\textbf{w}|^2$.\n",
    "Es ergibt sich also das Minimierungsproblem:\n",
    "$$\n",
    "\\text{min: }J(\\textbf{w},b) = \\frac{1}{2}|\\textbf{w}|^2\\\\\\text{wobei gleichzeitig für alle } \\textbf{x}_i \\text{ auch } y_i(\\textbf{w}^T\\textbf{x}_i + b) \\geq 1 \\text{ gelten muss.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Minimierung einer Funktion mit Nebenbedingungen kann die **Langrange-Methode** verwednet werden.\n",
    "Dazu stellen wir die folgende allgemeine **Lagrange-Funktion**:\n",
    "$$\\mathcal{L}(\\textbf{w},b,\\alpha)=\\frac{1}{2}|\\textbf{w}|^2-\\sum_{i=1}^l\\alpha_i[y_i(\\textbf{w}\\ \\textbf{x_i} + b) - 1]; \\ \\ \\alpha_i \\geq 0$$\n",
    "Die partiellen Ableitungen von $L$ müssen nun gebildet und auf Null gesetzt werden, um die möglichen Extremstellen zu bestimmen:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\textbf{w}} = \\textbf{w} - \\sum_{i=1}^l\\alpha_iy_i\\textbf{x_i}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_{i=1}^l\\alpha_iy_i$$\n",
    "\n",
    "Für die möglichen Minima der Funktion muss also gelten:\n",
    "\n",
    "$$\\textbf{w} = \\sum_{i=1}^l\\hat{\\alpha_i}y_i\\textbf{x_i} \\ \\text{ und } \\sum_{i=1}^l\\alpha_iy_i=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setzt man diese Bedingungen in die ursprüngliche Lagrange-Funktion ein, so erhält man folgenden Term:\n",
    "\n",
    "$$\\mathcal{L}(\\hat{\\textbf{w}},\\hat{b},\\alpha)=\\frac{1}{2}  \\sum_{i=1}^l\\sum_{j=1}^l\\alpha_i\\alpha_jy_iy_j\\textbf{w_i}^T\\textbf{w_j} - \\sum_{i=1}^l \\alpha_i$$\n",
    "$$ \\text{mit } \\alpha_i \\geq 0 \\text{ für alle } i=1,2,\\dots ,l$$\n",
    "\n",
    "Über diese Gleichung kann ein $\\hat{\\alpha}$ gefunden werden, dass die Funktion $\\mathcal{L}$ minimiert. Sobald man $\\hat{\\alpha}$ kennt, kann man das Ergebnis in die Gleichung\n",
    "\n",
    "$$\\hat{\\textbf{w}} = \\sum_{i=1}^l\\hat{\\alpha_i}y_i\\textbf{x_i}$$\n",
    "Über einen gefunden Stützvektor kann dann den *Bias-Term* $\\hat{b}$ bestimmt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Die Minimierung über die Lagrange Funktion ist mathematisch recht komplex. Wir können aber folgende Erkenntnisse festhalten:\n",
    "- Das Lösen des Optimierungsproblems basiert allein auf der Berechnung von Skalarprodukten der Form $\\textbf{x_i}\\ \\textbf{x_j}$\n",
    "- Das Schätzen der Klasse eines neuen Datenpunktes $\\textbf{u}$ erfolgt über die Funktion $\\text{sgn}[\\hat{\\textbf{w}}^T \\textbf{u} + \\hat{b}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_a(a,size):\n",
    "    # fitting svc model\n",
    "    X_slice, y_slice, y_col_slice = X[:size], y[:size], y_col[:size]\n",
    "    model = SVC(kernel='linear', C=a).fit(X_slice, y_slice)\n",
    "\n",
    "    # plotting data\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2,figsize=(6, 3.5), gridspec_kw={'width_ratios':[4, 1]})\n",
    "    for i, s, m, label in [(0, 100, 'o', 'Negative'), (1, 80, 's', 'Positive')]:\n",
    "        ax1.scatter(X_slice[y_slice==i,0], X_slice[y_slice==i,1], c=y_col_slice[y_slice==i],\n",
    "                    s=s, marker = m, edgecolors='black', label=label)\n",
    "    ax1.scatter(model.support_vectors_[:, 0],\n",
    "           model.support_vectors_[:, 1],\n",
    "           s=400, edgecolors='black', facecolors='none')\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(0, 1, 30)\n",
    "    yy = np.linspace(0, 1, 30)\n",
    "    yy, xx = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(xx.shape)\n",
    "\n",
    "    # plotting decision boundry\n",
    "    ax1.contour(xx, yy, P, colors='k', levels=[-1, 0, 1],\n",
    "                alpha=0.8, linestyles=['--', '-', '--'])\n",
    "    \n",
    "    ax1.axis('off')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # bar graph for lagrangian multipliers\n",
    "    a_list = ['a'+str(a) for a in range(model.dual_coef_.size)]\n",
    "    ax2.barh(list(a_list), np.abs(model.dual_coef_).squeeze(), color='yellow')\n",
    "    ax2.scatter(X[0], X[1])\n",
    "    \n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "interact(plt_a, a= IntSlider(value=20, min=1, max=20, step=1) , \n",
    "                 size= IntSlider(value=50, min=20, max=50, step=5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen:\n",
    "[1] M. Berthold, C. Borgelt, F. Höppner and F. Klawonn, *Guide to Intelligent Data Analysis*, Springer, 2010.\\\n",
    "[2] Jake VanderPlas, [*Python Data Science Handbook*](https://jakevdp.github.io/PythonDataScienceHandbook), O'Reilly, 2016.\\\n",
    "[3] Patrick Winston. *6.034 Artificial Intelligence*. MIT OpenCourseWare, 2010, https://ocw.mit.edu. License: Creative Commons BY-NC-SA.\n",
    "\n",
    "### Web-Quellen:\n",
    "1. [Understanding the mathematics behind Support Vector Machines](https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/)\n",
    "2. [Support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
